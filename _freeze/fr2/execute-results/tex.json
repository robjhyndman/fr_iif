{
  "hash": "a4a7b47f3debc968a81998521830b448",
  "result": {
    "markdown": "---\ntitle: Forecast reconciliation\nsubtitle: 2. Perspectives on forecast reconciliation\nauthor: Rob J Hyndman\npdf-engine: pdflatex\nfig-width: 9\nfig-height: 4.5\nformat:\n  beamer:\n    theme: monash\n    aspectratio: 169\n    fontsize: 14pt\n    section-titles: false\n    knitr:\n      opts_chunk:\n        dev: \"CairoPDF\"\ninclude-in-header: header.tex\ncite-method: biblatex\nbibliography: hts.bib\nbiblio-title: References\nkeep-tex: true\nexecute:\n  echo: false\n  message: false\n  warning: false\n  cache: true\n---\n\n::: {.cell hash='fr2_cache/beamer/unnamed-chunk-1_ad544df08a470aa6a8c8a4d0296a9126'}\n\n:::\n\n\n## Outline\n\n\\vspace*{0.4cm}\\tableofcontents\n\n#\n\n# Reconciliation via constraints\n\n## Notation reminder\n\n\\begin{textblock}{8.5}(0.2,1.5)\nEvery collection of time series with linear constraints can be written as\n\\centerline{\\colorbox[RGB]{210,210,210}{$\\bY_{t}=\\color{blue}\\bS\\color{red}\\bm{b}_{t}$}}\n\\vspace*{-0.9cm}\\begin{itemize}\\parskip=0cm\\itemsep=0cm\n\\item $\\by_t=$ vector of all series at time $t$\n\\item $ y_{\\text{Total},t}= $ aggregate of all series at time\n$t$.\n\\item $ y_{X,t}= $ value of series $X$ at time $t$.\n\\item $\\color{red}{\\bm{b}_t}=$ vector of most disaggregated series at time $t$\n\\item $\\color{blue}{\\bS}=$ ``summing matrix'' containing the linear constraints.\n\\end{itemize}\n\\end{textblock}\n\n\\begin{textblock}{5.7}(11.4,0.1)\n\\begin{minipage}{4cm}\n\\begin{block}{}\\centering\n\\begin{tikzpicture}\n\\tikzstyle{every node}=[ellipse,draw,fill=red!15,inner sep=2pt]\n\\tikzstyle[level distance=.3cm]\n\\tikzstyle[sibling distance=12cm]\n\\tikzstyle{level 1}=[sibling distance=10mm,font=\\small,set style={{every node}+=[fill=blue!15]}]\n\\node{Total}[edge from parent fork down]\n child {node {A}\n }\n child {node {B}\n }\n child {node {C}\n };\n\\end{tikzpicture}\n\\end{block}\n\\end{minipage}\n\\end{textblock}\n\n\\only<1>{\\begin{textblock}{5.7}(9.4,2.8)\\fontsize{14}{15}\\sf\n\\begin{align*}\n\\bY_{t}&= \\begin{pmatrix}\n  y_{\\text{Total},t}\\\\\n  y_{A,t}\\\\\n  y_{B,t}\\\\\n  y_{C,t}\n  \\end{pmatrix}  \\\\\n  &= {\\color{blue}\\underbrace{\\begin{pmatrix}\n                1 & 1 & 1 \\\\\n                1 & 0 & 0 \\\\\n                0 & 1 & 0\\\\\n                0 & 0 & 1\n                \\end{pmatrix}}_{\\bS}}\n     {\\color{red}\\underbrace{\\begin{pmatrix}\n       y_{A,t}\\\\y_{B,t}\\\\y_{C,t}\n       \\end{pmatrix}}_{\\bm{b}_{t}}}\n\\end{align*}\n\\end{textblock}}\n\\only<2>{\\begin{textblock}{5.7}(9.4,3.2)\\fontsize{14}{15}\\sf\n\\begin{alertblock}{}\n\\begin{itemize}\\itemsep=0.1cm\n\\item Base forecasts: $\\hat{\\bm{y}}_{T+h|T}$\n\\item Reconciled forecasts: $\\tilde{\\bm{y}}_{T+h|T}=\\bS\\bm{G}\\hat{\\bm{y}}_{T+h|T}$\n\\item MinT: $\\bG = (\\bS'\\bm{W}_h^{-1}\\bS)^{-1}\\bS'\\bm{W}_h^{-1}$ where $\\bm{W}_h$ is covariance matrix of base forecast errors.\n\\end{itemize}\n\\end{alertblock}\n\\end{textblock}}\n\n## Notation\n\n\\begin{textblock}{5.7}(11.4,0.1)\n\\begin{minipage}{4cm}\n\\begin{block}{}\\centering\n\\begin{tikzpicture}\n\\tikzstyle{every node}=[ellipse,draw,fill=red!15,inner sep=2pt]\n\\tikzstyle[level distance=.3cm]\n\\tikzstyle[sibling distance=12cm]\n\\tikzstyle{level 1}=[sibling distance=10mm,font=\\small,set style={{every node}+=[fill=blue!15]}]\n\\node{Total}[edge from parent fork down]\n child {node {A}\n }\n child {node {B}\n }\n child {node {C}\n };\n\\end{tikzpicture}\n\\end{block}\n\\end{minipage}\n\\end{textblock}\n\n\\begin{textblock}{6}(0.5,1.5)\n\\begin{block}{Aggregation matrix}\\vspace*{-0.6cm}\n\\begin{align*}\n\\bY_{t} & =\\color{blue}\\bS\\color{red}\\bm{b}_{t} \\\\[0.3cm]\n\\begin{pmatrix}\n   \\textcolor{DarkYellow}{y_{\\text{Total},t}}\\\\\n   \\textcolor{red}{y_{A,t}}\\\\\n   \\textcolor{red}{y_{B,t}}\\\\\n   \\textcolor{red}{y_{C,t}}\n  \\end{pmatrix}\n  &= {\\color{blue}\\begin{pmatrix}\n                \\textcolor{DarkYellow}1 & \\textcolor{DarkYellow}1 & \\textcolor{DarkYellow}1 \\\\\n                1 & 0 & 0 \\\\\n                0 & 1 & 0\\\\\n                0 & 0 & 1\n                \\end{pmatrix}}\n     {\\color{red}\\begin{pmatrix}\n       y_{A,t}\\\\y_{B,t}\\\\y_{C,t}\n       \\end{pmatrix}} \\\\[0.2cm]\n  \\begin{pmatrix}\\textcolor{DarkYellow}{\\bm{a}_t}\\\\\\textcolor{red}{\\bm{b}_t}\\end{pmatrix}\n   & = \\begin{pmatrix}\\textcolor{DarkYellow}{\\bm{A}}\\\\\\bm{I}_{n_b}\\end{pmatrix}\\textcolor{red}{\\bm{b}_t}\n\\end{align*}\n\\end{block}\n\\end{textblock}\n\n\\begin{textblock}{7}(8.1,3.5)\n\\begin{block}{Constraint matrix}\\vspace*{-0.6cm}\n\\begin{align*}\n\\bm{C} \\bY_t & = \\bm{0} \\\\\n\\text{where}\\qquad\n\\bm{C} & =  \\begin{bmatrix} 1 & -1 & -1 & -1 \\end{bmatrix} \\\\\n  & = \\begin{bmatrix} \\bm{I}_{n_a} & -\\textcolor{DarkYellow}{\\bm{A}} \\end{bmatrix}\n\\end{align*}\n\\end{block}\n\\end{textblock}\n\n\n## Zero-constraint representation\n\\vspace*{0.2cm}\n\n\\begin{block}{Aggregation matrix $\\bm{A}$}\n$$\\bm{y}_t\n    = \\begin{bmatrix}\\bm{a}_t\\\\\\bm{b}_t\\end{bmatrix}\n    = \\begin{bmatrix}\\bm{A}\\\\\\bm{I}_{n_b}\\end{bmatrix}\\bm{b}_t\n    = \\bm{S}\\bm{b}_t\n$$\n\\end{block}\\pause\n\n\\begin{block}{Constraint matrix $\\bm{C}$}\n\\centerline{$\\bm{C}\\bm{y}_t = \\bm{0}$}\n\\end{block}\\vspace*{-0.3cm}\n\n* Constraint matrix approach more general & more parsimonious.\n* $\\bm{C} = [\\bm{I}_{n_a} ~~~ {-\\bm{A}}]$.\n* $\\bm{S}$, $\\bm{A}$ and $\\bm{C}$ may contain any real values (not just 0s and 1s).\n\n## Zero-constraint representation\n\nAssuming $\\bm{C}$ is full rank\n\\begin{block}{}\n\\centerline{$\\tilde{\\bm{y}}_{T+h|T} = \\bm{M}\\hat{\\bm{y}}_{T+h|T}$}\n\\centerline{where\\qquad $\\bm{M} = \\bm{I} - \\bm{W}_h\\bm{C}'(\\bm{C}\\bm{W}_h\\bm{C}')^{-1}\\bm{C}$}\n\\end{block}\\vspace*{-0.3cm}\n\n* Originally proved by @Byron1978 & @Byron1979 for reconciling data.\n* Re-discovered by @mint for reconciling forecasts.\n* $\\bm{M} = \\bm{S}\\bm{G}$ (the MinT solution)\n* Leads to more efficient reconciliation than using $\\bm{G}$.\n\n## Zero-constraint representation\n\n\\begin{textblock}{5.7}(11.4,0.1)\n\\begin{minipage}{4cm}\n\\begin{block}{}\\centering\n\\begin{tikzpicture}\n\\tikzstyle{every node}=[ellipse,draw,fill=red!15,inner sep=2pt]\n\\tikzstyle[level distance=.3cm]\n\\tikzstyle[sibling distance=12cm]\n\\tikzstyle{level 1}=[sibling distance=10mm,font=\\small,set style={{every node}+=[fill=blue!15]}]\n\\node{Total}[edge from parent fork down]\n child {node {A}\n }\n child {node {B}\n }\n child {node {C}\n };\n\\end{tikzpicture}\n\\end{block}\n\\end{minipage}\n\\end{textblock}\n\n\\begin{textblock}{5.8}(9.6,3.5)\n\\begin{block}{}\\vspace*{-0.6cm}\\fontsize{11}{11}\\sf\n\\begin{align*}\n\\bm{A} &= \\begin{pmatrix}\n           1 & 1 & 1\n           \\end{pmatrix} \\\\\n\\bS & = \\begin{pmatrix}\\bm{A}\\\\\\bm{I}_{n_b}\\end{pmatrix} =\n\\begin{pmatrix}\n                1 & 1 & 1 \\\\\n                1 & 0 & 0 \\\\\n                0 & 1 & 0\\\\\n                0 & 0 & 1\n                \\end{pmatrix} \\\\\n\\bm{C} & = \\begin{pmatrix} \\bm{I}_{n_a} & - \\bm{A} \\end{pmatrix} = \\begin{pmatrix} 1 & -1 & -1 & -1 \\end{pmatrix}\n\\end{align*}\n\\end{block}\n\\end{textblock}\n\n\\begin{textblock}{9}(.2, 1.5)\\fontsize{11}{11}\\sf\nSuppose $\\bm{W}_h = \\bm{I}$. Then\\vspace*{-0.2cm}\n\\begin{align*}\n\\bm{M} &= \\bm{I} - \\bm{W}_h\\bm{C}'(\\bm{C}\\bm{W}_h\\bm{C}')^{-1}\\bm{C} \\\\\n       &= \\begin{pmatrix}\n                1 & 0 & 0 & 0 \\\\\n                0 & 1 & 0 & 0 \\\\\n                0 & 0 & 1 & 0\\\\\n                0 & 0 & 0 & 1\n                \\end{pmatrix} -\n                \\begin{pmatrix} \\phantom{-}1 \\\\ -1 \\\\ -1 \\\\ -1 \\end{pmatrix} \\frac{1}{4} \\begin{pmatrix} 1 & -1 & -1 & -1 \\end{pmatrix} \\\\\n&= \\begin{pmatrix}\n                1 & 0 & 0 & 0 \\\\\n                0 & 1 & 0 & 0 \\\\\n                0 & 0 & 1 & 0\\\\\n                0 & 0 & 0 & 1\n                \\end{pmatrix} -\n                \\begin{pmatrix}\n                \\phantom{-}1 & -1 & -1 & -1 \\\\\n                -1 & \\phantom{-}1 & \\phantom{-}1 & \\phantom{-}1 \\\\\n                -1 & \\phantom{-}1 & \\phantom{-}1 & \\phantom{-}1 \\\\\n                -1 & \\phantom{-}1 & \\phantom{-}1 & \\phantom{-}1\n                \\end{pmatrix} \\\\\n                & =\n                \\begin{pmatrix}\n                \\frac{3}{4} & \\phantom{-}\\frac14 & \\phantom{-}\\frac14 & \\phantom{-}\\frac14 \\\\[0.1cm]\n                \\frac{1}{4} & \\phantom{-}\\frac34 & -\\frac14 & -\\frac14 \\\\[0.1cm]\n                \\frac{1}{4} & -\\frac14 & \\phantom{-}\\frac34 & -\\frac14  \\\\[0.1cm]\n                \\frac{1}{4} & -\\frac14 & -\\frac14 & \\phantom{-}\\frac34\n                \\end{pmatrix}\n\\end{align*}\n\\end{textblock}\n\n\n# The geometry of forecast reconciliation\n\n## The coherent subspace\n\n\\begin{textblock}{9}(.2,1.25)\\fontsize{13}{13}\\sf\n\\begin{block}{Coherent subspace}\n$n_b$-dimensional linear subspace $\\mathfrak{s}\\subset \\mathbb{R}^n$ for which linear constraints hold for all $\\bm{y}\\in\\mathfrak{s}$.\n\\end{block}\\vspace*{-0.3cm}\n\\begin{block}{Hierarchical time series}\nAn $n$-dimensional multivariate time series such that $\\bm{y}_t\\in\\mathfrak{s}\\quad\\forall t$.\n\\end{block}\\vspace*{-0.3cm}\n\\begin{block}{Coherent point forecasts}\n$\\tilde{\\bm{y}}_{t+h|t}$ is \\emph{coherent} if $\\tilde{\\bm{y}}_{t+h|t} \\in \\mathfrak{s}$.\n\\end{block}\\vspace*{-0.2cm}\n\\end{textblock}\n\\only<2-3>{\\begin{textblock}{7.5}(.2,6.75)\\fontsize{13}{13}\\sf\n\\begin{alertblock}{Base forecasts}\nLet $\\hat{\\bm{y}}_{t+h|t}$ be vector of \\emph{incoherent} initial $h$-step forecasts.$\\phantom{y_{t|h}}$\n\\end{alertblock}\n\\end{textblock}}\n\\only<3>{\\begin{textblock}{7.5}(8.3,6.75)\\fontsize{13}{13}\\sf\n\\begin{alertblock}{Reconciled forecasts}\nLet $\\psi$ be a mapping, $\\psi:\\mathbb{R}^n\\rightarrow\\mathfrak{s}$.  $\\tilde{\\bm{y}}_{t+h|t}=\\psi(\\hat{\\bm{y}}_{t+h|t})$ ``reconciles'' $\\hat{\\bm{y}}_{t+h|t}$.\n\\end{alertblock}\n\\end{textblock}}\n\n\\placefig{9.4}{.0}{width=6.6cm}{3D_hierarchy}\n\\begin{textblock}{3}(11.4,5.6)\\fontsize{13}{13}\\sf\n\\begin{block}{}\n\\centerline{$ y_{Tot} = y_A + y_B$}\n\\end{block}\n\\end{textblock}\n\n## The coherent subspace\n\n\\begin{textblock}{5.7}(11.4,0.1)\n\\begin{minipage}{4cm}\n\\begin{block}{}\\centering\n\\begin{tikzpicture}\n\\tikzstyle{every node}=[ellipse,draw,fill=red!15,inner sep=2pt]\n\\tikzstyle[level distance=.3cm]\n\\tikzstyle[sibling distance=12cm]\n\\tikzstyle{level 1}=[sibling distance=10mm,font=\\small,set style={{every node}+=[fill=blue!15]}]\n\\node{Total}[edge from parent fork down]\n child {node {A}\n }\n child {node {B}\n }\n child {node {C}\n };\n\\end{tikzpicture}\n\\end{block}\n\\end{minipage}\n\\end{textblock}\n\nThe columns of $\\bm{S}$ form a basis set for $\\mathfrak{s}$.\n\nThey are not unique.\\newline Each corresponds to different vector of \"bottom-level\" series.\n\n\\only<2>{\\begin{block}{}\n\\centerline{$\\displaystyle\n\\bm{y} = \\begin{pmatrix}\\text{Total}\\\\A\\\\B\\\\C\\end{pmatrix}\\qquad\n\\bS  = \\begin{pmatrix*}[r]\n                \\phantom{-}1 & \\phantom{-}1 & \\phantom{-}1 \\\\\n                1 & 0 & 0 \\\\\n                0 & 1 & 0\\\\\n                0 & 0 & 1\n                \\end{pmatrix*} \\qquad\n  \\bm{b} = \\begin{pmatrix}\\phantom{Total}\\\\[-0.7cm]\n                A \\\\\n                B \\\\\n                C \\\\\n                \\end{pmatrix}\n$}\n\\end{block}}\n\\only<3>{\\begin{block}{}\n\\centerline{$\\displaystyle\n\\bm{y} = \\begin{pmatrix}\\text{Total}\\\\A\\\\B\\\\C\\end{pmatrix}\\qquad\n\\bS  =\n\\begin{pmatrix*}[r]\n                \\phantom{-}1 & 0 & 0 \\\\\n                0 & 0 & 1 \\\\\n                0 & 1 & 0\\\\\n                1 & -1 & -1\n                \\end{pmatrix*}\\qquad\n  \\bm{b} = \\begin{pmatrix}\n      \\text{Total} \\\\\n      B \\\\\n      A\n  \\end{pmatrix}\n$}\n\\end{block}}\n\\only<4>{\\begin{block}{}\n\\centerline{$\\displaystyle\n\\bm{y} = \\begin{pmatrix}\\text{Total}\\\\A\\\\B\\\\C\\end{pmatrix}\\qquad\n\\bS  =\n\\begin{pmatrix*}[r]\n                1 & 0 & 0 \\\\\n                1 & 0 & -1 \\\\\n                -1 & 1 & 1\\\\\n                1 & -1 & 0\n                \\end{pmatrix*}\\qquad\n  \\bm{b} = \\begin{pmatrix}\n      \\text{Total} \\\\\n      B+A \\\\\n      C+B\n  \\end{pmatrix}\n$}\n\\end{block}}\n\n\\vspace*{10cm}\n\n## Projections in linear algebra\n\n\\begin{textblock}{9}(.2,1.25)\n\\begin{tikzpicture}\n  % Define coordinates\n  \\coordinate (O) at (0,0);\n  \\coordinate (A) at (4,2);\n  \\coordinate (B) at (2,3);\n  \\coordinate (L) at (1,5);\n\n  % Draw axes\n  \\draw[->,thick] (0,0) -- (5,0) node[anchor=north] {$x$};\n  \\draw[->,thick] (0,0) -- (0,4) node[anchor=east] {$y$};\n\n  % Draw light\n  \\path [fill = yellow!30!white] (L) circle (.9cm);\n  \\path [fill = yellow!60!white] (L) circle (.6cm);\n  \\path [fill = yellow!90!white] (L) circle (.3cm);\n\n  % Draw vectors\n  \\draw[->,thick,red] (O) -- (A);\n  \\draw[->,thick,blue] (O) -- (B);\n\n  % Draw projection\n  \\coordinate (P) at ($(A)!(B)!(O)$);\n  \\only<2->{\\draw[->,thick,blue,dashed] (B) -- (P);}\n  \\only<2->{\\draw[->,thick,blue, dashed] (O) -- (P);}\n\\end{tikzpicture}\n\\end{textblock}\n\n\n\n\n\n\\begin{textblock}{10}(7,1.6)\n\\only<3>{\\includegraphics[width=8.8cm]{scatterplot1}}\n\\only<4>{\\includegraphics[width=8.8cm]{scatterplot2}}\n\\only<5>{\\includegraphics[width=8.8cm]{scatterplot3}}\n\\only<6>{\\includegraphics[width=8.8cm]{scatterplot4}}\n\\end{textblock}\n\n## Projections in linear algebra\n\n* A projection is a linear transformation $\\bm{M}$ such that $\\bm{M}^2=\\bm{M}$.\n* i.e., $M$ is idempotent: it leaves its image unchanged.\n* $\\bm{M}$ projects onto $\\mathfrak{s}$ if $\\bm{M}\\bm{y}=\\bm{y}$ for all $\\bm{y}\\in\\mathfrak{s}$.\n* All eigenvalues of $\\bm{M}$ are either 0 or 1.\n* All singular values of $\\bm{M}$ are greater than or equal to 1 (with equality iff $\\bm{M}$ is orthogonal).\n* A projection is *orthogonal* if $\\bm{M}'=\\bm{M}$.\n* If a projection is not orthogonal, it is called *oblique*.\n* In regression, OLS is an orthogonal projection onto space spanned by predictors.\n\n## Linear projection reconciliation\n\n\\only<1>{\\placefig{9.5}{1.5}{width=6.2cm}{InsampDir_2_George}}\n\\only<2>{\\placefig{9.5}{1.5}{width=6.2cm}{OrthProj_George}}\n\\only<3->{\\placefig{9.5}{1.5}{width=6.2cm}{ObliqProj_George}}\n\n\\begin{textblock}{9}(.5,1.5)\n  \\begin{itemize}\\tightlist\n  \\item $R$ is the most likely direction of deviations from $\\mathfrak{s}$.\n  \\only<1->{\\item Grey: potential base forecasts}\n  \\only<2->{\\item Red: reconciled forecasts}\n  \\only<2->{\\item Orthogonal projections (i.e., OLS) lead to smallest possible adjustments of base forecasts.}\n  \\only<3->{\\item Oblique projections (i.e., MinT) give reconciled forecasts with smallest variance.}\n  \\end{itemize}\n\\end{textblock}\n\n\\only<2->{\n  \\begin{textblock}{4.5}(11.2,7.6)\\fontsize{13}{14}\\sf\n  \\begin{block}{}\n  \\only<2>{Orthogonal projection}\n  \\only<3>{Oblique projection}\n  \\end{block}\n  \\end{textblock}\n}\n\n## Linear projection reconciliation\n\\fontsize{14}{16}\\sf\n\\vspace*{0.2cm}\\begin{alertblock}{}\n\\centerline{$\\tilde{\\bm{y}}_{t+h|t}= \\psi(\\hat{\\bm{y}}_{t+h|t}) = \\bm{M}\\hat{\\bm{y}}_{t+h|t}$}\n\\end{alertblock}\\vspace*{-0.5cm}\n\n* $\\bm{M}$ is a projection onto $\\mathfrak{s}$ if and only if $\\bm{M}\\bm{y}=\\bm{y}$ for all $\\bm{y}\\in\\mathfrak{s}$.\n* Coherent base forecasts are unchanged since $\\bm{M}\\hat{\\bm{y}}=\\hat{\\bm{y}}$\n* If $\\hat{\\bm{y}}$ is unbiased, then $\\tilde{\\bm{y}}$ is also unbiased since\n$$\n  \\E(\\tilde{\\bm{y}}_{t+h|t}) = \\E(\\bm{M}\\hat{\\bm{y}}_{t+h|t}) = \\bm{M} \\E(\\hat{\\bm{y}}_{t+h|t}) = \\E(\\hat{\\bm{y}}_{t+h|t}),\n$$\nand unbiased estimates must lie on $\\mathfrak{s}$.\n* The projection is orthogonal if and only if $\\bm{M}'=\\bm{M}$.\n* If $\\bm{S}$ forms a basis set for $\\mathfrak{s}$, then projections are of the form $\\bm{M} = \\bS(\\bS'\\bm{\\Psi}\\bS)^{-1}\\bS'\\bm{\\Psi}$ where $\\bm{\\Psi}$ is a positive definite matrix.\n\n\\vspace*{10cm}\n\n## Linear projection reconciliation\n\n\\vspace*{0.2cm}\\begin{alertblock}{}\n\\centerline{$\\tilde{\\bm{y}}_{t+h|t}= \\psi(\\hat{\\bm{y}}_{t+h|t}) = \\bm{M}\\hat{\\bm{y}}_{t+h|t},\\qquad\\text{where}\\quad \\bm{M} = \\bS(\\bS'\\bm{\\Psi}\\bS)^{-1}\\bS'\\bm{\\Psi}$}\n\\end{alertblock}\\vspace*{.01cm}\n\\begin{block}{}\\vspace*{-0.6cm}\n\\begin{align*}\n&\\text{OLS:}  && \\bm{\\Psi}=\\bm{I} &&& \\bm{M} & = \\bm{S}(\\bm{S}'\\bm{S})^{-1}\\bm{S}' && = \\bm{I} - \\bm{C}'(\\bm{C}\\bm{C}')^{-1}\\bm{C} \\\\\n&\\text{MinT:} && \\bm{\\Psi}=\\bm{W}_h &&&\\bm{M} & = \\bS(\\bS'\\bm{W}_h^{-1}\\bS)^{-1}\\bS'\\bm{W}_h^{-1} && = \\bm{I} - \\bm{W}_h\\bm{C}'(\\bm{C}\\bm{W}_h\\bm{C}')^{-1}\\bm{C}\n\\end{align*}\n\\end{block}\\vspace*{-0.3cm}\n\n* $\\bm{M}$ is orthogonal iff $\\bm{\\Psi}=\\bm{I}$.\n* $\\bm{W}_h = \\var[\\by_{T+h} - \\hat{\\by}_{T+h|T} \\mid \\by_1,\\dots,\\by_T]$ is the covariance matrix of the base forecast errors.\n* $\\bm{V}_h = \\var[\\by_{T+h} - \\tilde{\\by}_{T+h|T}  \\mid \\by_1,\\dots,\\by_T]  = \\bm{M}\\bm{W}_h\\bm{M}'$ is minimized when  $\\bm{\\Psi} = \\bm{W}_h$.\n\n\\vspace*{10cm}\n\n## Mean square error bounds\n\n\\begin{textblock}{6.4}(9,0.)\\begin{block}{}\n\\citet{htsgeometry}\n\\end{block}\\end{textblock}\n\n\\vspace*{0.2cm}\\begin{alertblock}{Distance reducing property}\nLet $\\|\\bm{u}\\|_{\\bm{\\Psi}} = \\bm{u}'\\bm{\\Psi}\\bm{u}$. Then\n  \\centerline{$\\|\\bm{y}_{t+h}-\\tilde{\\bm{y}}_{t+h|t}\\|_{\\bm{\\Psi}}\\le\\|\\bm{y}_{t+h}-\\hat{\\bm{y}}_{t+h|t}\\|_{\\bm{\\Psi}}$}\n\\end{alertblock}\n\n * $\\bm{\\Psi}$-projection is guaranteed to improve forecast accuracy over base forecasts *using this distance measure*.\n * Distance reduction holds for any realisation and any forecast.\n * OLS reconciliation minimizes Euclidean distance.\n * Other measures of forecast accuracy may be worse.\n\n## Mean square error bounds\n\n\\begin{textblock}{6.4}(9,0.)\\begin{block}{}\n\\citet{wickramasuriya2021properties}\n\\end{block}\\end{textblock}\n\n\\begin{block}{}\\vspace*{-0.6cm}\n\\begin{align*}\n\\|\\bm{y}_{t+h} - \\tilde{\\bm{y}}_{t+h}\\|_2^2\n &= \\|\\bm{M}(\\bm{y}_{t+h} - \\hat{\\bm{y}}_{t+h})\\|_2^2 \\\\\n &\\le \\|\\bm{M}\\|_2^2 \\|\\bm{y}_{t+h} - \\hat{\\bm{y}}_{t+h}\\|_2^2 \\\\\n & = \\sigma_{\\text{max}}^2\\|\\bm{y}_{t+h} - \\hat{\\bm{y}}_{t+h}\\|_2^2\n\\end{align*}\n\\end{block}\n\n * $\\sigma_{\\text{max}}$ is the largest eigenvalue of $\\bm{M}$\n * $\\sigma_{\\text{max}}\\ge1$ as $\\bm{M}$ is a projection matrix.\n * Every projection reconciliation is better than base forecasts using Euclidean distance.\n\n## Mean square error bounds\n\n\\begin{textblock}{6.4}(9,0.)\\begin{block}{}\n\\citet{wickramasuriya2021properties}\n\\end{block}\\end{textblock}\n\\vspace*{0.2cm}\\begin{block}{}\\vspace*{-0.6cm}\n\\begin{align*}\n    & \\text{tr}\\Big(\\E[\\bm{y}_{t+h} - \\tilde{\\bm{y}}^{\\text{MinT}}_{t+h|t}]'[\\bm{y}_{t+h} - \\tilde{\\bm{y}}^{\\text{MinT}}_{t+h|t}]\\Big) \\\\\n\\le~ & \\text{tr}\\Big(\\E[\\bm{y}_{t+h} - \\tilde{\\bm{y}}^{\\text{OLS}}_{t+h|t}]'[\\bm{y}_{t+h} - \\tilde{\\bm{y}}^{\\text{OLS}}_{t+h|t}]\\Big) \\\\\n\\le~ & \\text{tr}\\Big(\\E[\\bm{y}_{t+h} - \\hat{\\bm{y}}_{t+h|t}]'[\\bm{y}_{t+h} - \\hat{\\bm{y}}_{t+h|t}]\\Big)\n\\end{align*}\n\\end{block}\n\nUsing sums of variances:\n\n* MinT reconciliation is better than OLS reconciliation\n* OLS reconciliation is better than base forecasts\n\n# Optimization and reconcilation\n\n## Minimum trace reconciliation\n\n\\vspace*{0.2cm}\\begin{alertblock}{Minimum trace (MinT) reconciliation}\nIf $\\bS\\bG$ is a projection, then the trace of $\\bm{V}_h = \\text{Var}(\\tilde{\\bm{y}}_{t+h|t} - \\bm{y}_{t+h})$ is \\textbf{minimized} when\n\\centerline{$\\bG = (\\bS'\\bm{W}_h^{-1}\\bS)^{-1}\\bS'\\bm{W}_h^{-1}$}\n\\end{alertblock}\n\\begin{block}{}\n\\centerline{$\\displaystyle\\textcolor{red}{\\tilde{\\by}_{T+h|T}}\n=\\bS(\\bS'\\bm{W}_h^{-1}\\bS)^{-1}\\bS'\\bm{W}_h^{-1}\\textcolor{blue}{\\hat{\\by}_{T+h|T}}$}\n\\end{block}\n\\centerline{\\hspace*{1cm}\\textcolor{red}{Reconciled forecasts}\\hfill\\textcolor{blue}{Base forecasts}\\hspace*{2cm}}\n\n* Trace of $\\bm{V}_h$ is sum of forecast variances.\n* MinT solution is $L_2$ **optimal** amongst linear unbiased forecasts.\n\n## A game theoretic perspective\n\\fontsize{14}{16}\\sf\n\n\\begin{textblock}{6.4}(9,0.)\\begin{block}{}\n\\citet{Van_ErvCug2015}\n\\end{block}\\end{textblock}\n\nFind the solution to the minimax problem\n$$\nV = \\mathop{min}_{\\tilde{\\bm{y}} \\in \\mathfrak{s}}\n\\mathop{max}_{\\bm{y}\\in \\mathfrak{s}}\n\\left\\{\\ell(\\bm{y},\\tilde{\\bm{y}}) -\n\\ell(\\bm{y},\\hat{\\bm{y}})\n\\right\\},\n$$\nwhere $\\ell$ is a loss function, and $\\mathfrak{s}$ is the coherent subspace.\n\n* $V\\le0$: reconciliation guaranteed to reduce loss.\n* If $\\ell(\\bm{y},\\tilde{\\bm{y}}) = \\|\\bm{y}- \\tilde{\\bm{y}}\\|_{\\bm{\\Psi}} = (\\bm{y}-\\tilde{\\bm{y}})'\\bm{\\Psi}(\\bm{y}-\\tilde{\\bm{y}})$, where $\\bm{\\Psi}$ is any symmetric pd matrix, then:\n\n  1. $\\tilde{\\bm{y}}=\\bm{S}(\\bm{S}'\\bm{\\Psi}\\bm{S})^{-1}\\bm{S}'\\bm{\\Psi}\\hat{\\bm{y}}$ will always improve upon the base forecasts;\n  2. The MinT solution $\\tilde{\\bm{y}}=\\bm{S}(\\bm{S}'\\bm{W}_h^{-1}\\bm{S})^{-1}\\bm{S}'\\bm{W}_h^{-1}\\hat{\\bm{y}}$ will optimise loss in expectation over any choice of $\\bm{\\Psi}$.\n\n## Biased reconciliation\n\n\\begin{textblock}{6.4}(9,0.)\\begin{block}{}\n\\citet{Ben_TaiEtAl2019}\n\\end{block}\\end{textblock}\n\nRegularized empirical risk minimization problem:\n$$\\min_{\\bm{G}} \\frac{1}{Nn}\\|\\bm{Y} - \\hat{\\bm{Y}}\\bm{G}'\\bm{S}'\\|_F + \\lambda \\|\\text{vec}{\\bm{G}}\\|_1,\n$$\n\n* $N=T-T_1-h+1$,\\quad $T_1$ is minimum training sample size\n* $\\|\\cdot\\|_F$ is the Frobenius norm\n* $\\bm{Y} = [\\bm{y}_{T_1+h}, \\dots, \\bm{y}_{T}]'$\n* $\\hat{\\bm{Y}} = [\\hat{\\bm{y}}_{T_1+h|T_1}, \\dots, \\hat{\\bm{y}}_{T|T-h}]'$\n* $\\lambda$ is a regularization parameter\n\nWhen $\\lambda=0$:\\qquad  $\\hat{\\bm{G}} = \\bm{B}'\\hat{\\bm{Y}}(\\hat{\\bm{Y}}'\\hat{\\bm{Y}})^{-1}$\nwhere $\\bm{B} = [\\bm{b}_{T_1+h}, \\dots, \\bm{b}_{T}]'$.\n\nReference: @Ben_TaiEtAl2019\n\n## MinT expressed as a regression\n\nSince $\\tilde{\\bm{b}}_{t+h|t} = (\\bm{S}'\\bm{W}_h^{-1}\\bm{S})^{-1}\\bm{S}'\\bm{W}_h^{-1}\\hat{\\bm{y}}_{t+h|t}$, we can write the MinT solution as a regression problem:\n\\begin{block}{}\\vspace*{-0.7cm}\n\\begin{align*}\n\\tilde{\\bm{b}}_{t+h|t} &= \\text{arg min}_{\\bm{b}}\\big[\\hat{\\bm{y}}_{t+h|t} - \\bm{S}\\bm{b}\\big]' \\bm{W}_h^{-1}\\big[\\hat{\\bm{y}}_{t+h|t} - \\bm{S}\\bm{b}\\big] \\\\\n& =\\text{arg min}_{\\bm{b}}\\big[\\bm{b}'\\bm{S}'\\bm{W}_h^{-1}\\bm{S}\\bm{b} - 2\\bm{b}'\\bm{S}'\\bm{W}_h^{-1}\\hat{\\bm{y}}_{t+h|t} +\n\\hat{\\bm{y}}_{t+h|t}'\\bm{W}_h^{-1}\\hat{\\bm{y}}_{t+h|t} \\big]\\\\\n& =\\text{arg min}_{\\bm{b}}\\big[\\bm{b}'\\bm{S}'\\bm{W}_h^{-1}\\bm{S}\\bm{b} - 2\\bm{b}'\\bm{S}'\\bm{W}_h^{-1}\\hat{\\bm{y}}_{t+h|t}\\big]\n\\end{align*}\n\\end{block}\\vspace*{-0.2cm}\n\n* MinT solution is equivalent to a GLS regression of $\\hat{\\bm{y}}_{t+h|t}$ on $\\bm{S}$ with covariance weights $\\bm{W}_h^{-1}$.\n* The estimated coefficients are the forecasts of the bottom level series.\n\n\\vspace*{10cm}\n\n## Non-negative forecasts\n\\fontsize{14}{15}\\sf\n\\vspace*{0.1cm}\\begin{alertblock}{}\n\\centerline{$\\min_{\\bm{G}_h}\\text{tr}\\Big(\\E[\\bm{y}_{t+h} - \\bm{S}\\bm{G}_h\\hat{\\bm{y}}_{t+h|t}]'[\\bm{y}_{t+h} - \\bm{S}\\bm{G}_h\\hat{\\bm{y}}_{t+h|t}]\\Big)$}\n\\centerline{such that $\\bm{b}_{t+h|t} = \\bm{G}_h\\hat{\\bm{y}}_{t+h|t} \\ge 0$}\n\\end{alertblock}\\pause\\vspace*{-0.2cm}\n\n\\begin{block}{Solve via quadratic programming:}\n$$\n  \\text{min}_{\\bm{b}} \\big[\\bm{b}'\\bS'\\bm{W}_h^{-1}\\bS\\bm{b} - 2 \\bm{b}'\\bS'\\bm{W}_h^{-1}\\hat{\\bm{y}}_{T+h|T}\\big] \\quad \\text{s.t.~~} \\bm{b} \\ge 0\n$$\n\\rightline{\\citep{nonnegmint}}\n\\end{block}\\pause\\vspace*{-0.1cm}\n\n### Set-negative-to-zero heuristic solution\n  * Negative reconciled forecasts at bottom level set to zero\n  * Remaining forecasts computed via aggregation\n\n\\rightline{\\citep{di2023spatio}}\n\n## Immutable forecasts\n\n\\begin{textblock}{6.4}(9,0.)\\begin{block}{}\n\\citet{ZhaEtAl2022}\n\\end{block}\\end{textblock}\n$\\displaystyle \\hat{\\bm{y}}_{t+h|t} = \\begin{bmatrix}\\hat{\\bm{a}}_{t+h|t} \\\\ \\hat{\\bm{v}}_{t+h|t} \\\\ \\hat{\\bm{u}}_{t+h|t}\\end{bmatrix} = \\begin{bmatrix} \\bm{A}_1 & \\bm{A}_2 \\\\ \\bm{I}_{n_b-k} & \\bm{O} \\\\ \\bm{O} & \\bm{I}_k \\end{bmatrix} \\begin{bmatrix}\\hat{\\bm{v}}_{t+h|t} \\\\ \\hat{\\bm{u}}_{t+h|t}\\end{bmatrix}$\\vspace*{-0.5cm}\n\nSuppose $\\hat{\\bm{u}}_{t+h|t}$ are fixed and let $\\hat{\\bm{w}}_{t+h|t} = \\begin{bmatrix}\\hat{\\bm{a}}_{t+h|t} - \\bm{A}_2 \\hat{\\bm{u}}_{t+h|t} \\\\ \\hat{\\bm{v}}_{t+h|t}\\end{bmatrix}$.\n\\begin{block}{Optimization problem}\n\\centerline{$\\displaystyle\\text{min}_{\\bm{v}}\\big[\\hat{\\bm{w}}_{t+h|t} - \\bm{A}_3\\bm{v}\\big]' \\bm{W}_{\\bm{v}}^{-1}\\big[\\hat{\\bm{w}}_{t+h|t} - \\bm{A}_3\\bm{v}\\big]\\qquad\\text{where}\\qquad\n\\bm{A}_3 = \\begin{bmatrix}\\bm{A}_1 \\\\\\bm{I}_{n_b-k}\\end{bmatrix}$}\nand $\\bm{W}_{\\bm{v}}$ contains elements of $\\bm{W}_h$ corresponding to $\\hat{\\bm{v}}_{t+h|t}$.\n\\end{block}\n\\vspace*{10cm}\n\n## Immutable forecasts\n\n\\begin{textblock}{6.4}(9,0.)\\begin{block}{}\n\\citet{ZhaEtAl2022}\n\\end{block}\\end{textblock}\n$\\displaystyle \\hat{\\bm{y}}_{t+h|t} = \\begin{bmatrix}\\hat{\\bm{a}}_{t+h|t} \\\\ \\hat{\\bm{v}}_{t+h|t} \\\\ \\hat{\\bm{u}}_{t+h|t}\\end{bmatrix} = \\begin{bmatrix} \\bm{A}_1 & \\bm{A}_2 \\\\ \\bm{I}_{n_b-k} & \\bm{O} \\\\ \\bm{O} & \\bm{I}_k \\end{bmatrix} \\begin{bmatrix}\\hat{\\bm{v}}_{t+h|t} \\\\ \\hat{\\bm{u}}_{t+h|t}\\end{bmatrix}$\\vspace*{-0.5cm}\n\nSuppose $\\hat{\\bm{u}}_{t+h|t}$ are fixed and let $\\hat{\\bm{w}}_{t+h|t} = \\begin{bmatrix}\\hat{\\bm{a}}_{t+h|t} - \\bm{A}_2 \\hat{\\bm{u}}_{t+h|t} \\\\ \\hat{\\bm{v}}_{t+h|t}\\end{bmatrix}$.\n\\begin{block}{Solve with non-negativity constraint}\n\\centerline{$\\displaystyle\\text{min}_{\\bm{v}}\\big[\\hat{\\bm{w}}_{t+h|t} - \\bm{A}_3\\bm{v}\\big]' \\bm{W}_{\\bm{v}}^{-1}\\big[\\hat{\\bm{w}}_{t+h|t} - \\bm{A}_3\\bm{v}\\big]\\qquad\\text{where}\\qquad\n\\bm{A}_3 = \\begin{bmatrix}\\bm{A}_1 \\\\\\bm{I}_{n_b-k}\\end{bmatrix}$}\n$$\\text{such that}\\qquad \\bm{A}_3 \\bm{v} \\ge \\begin{bmatrix}-\\bm{A}_2\\hat{\\bm{u}}_{t+h|t}\\\\ \\bm{0}\\end{bmatrix}$$\n\\end{block}\n\n# ML reconciliation\n## ML reconciliation\n\n\\placefig{7}{0}{width=9.cm, height=20cm}{mlflow}\n\n\\begin{textblock}{7}(0,1.2)\\fontsize{11}{12}\\sf\n\\begin{enumerate}\\tightlist\n\\item Split all series using time series cross-validation\n\\item For each training set, compute one-step-ahead forecasts for all series\n\\item For each bottom-level series, use RF or XGB to predict values using forecasts of all series as inputs\n\\item Forecast all series\n\\item For each bottom-level series, apply ML model to improve forecasts\n\\item Aggregate bottom-level forecasts to obtain forecasts for other series.\n\\end{enumerate}\n\\end{textblock}\n\\begin{textblock}{6,5}(0.5,7.8)\\fontsize{11}{12}\\sf\n\\begin{block}{}\nSpiliotis et al. (ASC, 2021)\\nocite{hfrml}\n\\end{block}\n\\end{textblock}\n\n## ML reconciliation: tourism data\n\n\\fontsize{11}{11}\\sf\\hspace*{-0.8cm}\n\\begin{tabular}{l c c c c c}\n    \\textbf{Method} & \\textbf{Total} & \\textbf{States} & \\textbf{Zones} & \\textbf{Regions} & \\textbf{Average} \\\\\n    \\hline\n    \\multicolumn{6}{c}{MASE} \\\\\n    \\hline\n    MinT-Struct          & 1.094            & 0.968            & 0.887            & 0.843            & 0.948 \\\\\n    MinT-Shrink & 1.047 & \\textbf{0.956} & 0.872 & 0.824 & 0.925 \\\\\n    ML-RF           & 1.045            & 0.964            & 0.859            & 0.812            & 0.920 \\\\\n    ML-XGB          & \\textbf{1.043}   & 0.965            & \\textbf{0.859}   & \\textbf{0.812}   & \\textbf{0.920} \\\\\n    \\hline\n    \\multicolumn{6}{c}{RMSSE} \\\\\n    \\hline\n    MinT-Struct          & 1.308            & 1.225            & 1.137            & 1.109            & 1.195 \\\\\n    MinT-Shrink         & 1.265            & 1.214            & 1.120            & 1.086            & 1.171 \\\\\n    ML-RF           & 1.261            & \\textbf{1.208}   & 1.104            & 1.066            & 1.159 \\\\\n    ML-XGB          & 1.255            & 1.208            & \\textbf{1.101}   & \\textbf{1.064}   & \\textbf{1.157} \\\\\n    \\hline\n    \\multicolumn{6}{c}{AMSE} \\\\\n    \\hline\n    MinT-Struct          & 0.988            & 0.611            & 0.426            & 0.349            & 0.593 \\\\\n    MinT-Shrink         & 0.935            & 0.599            & 0.417            & 0.337            & 0.572 \\\\\n    ML-RF           & 0.780            & \\textbf{0.526}   & 0.366            & 0.319            & 0.498 \\\\\n    ML-XGB          & \\textbf{0.779}   & 0.526            & \\textbf{0.365}   & \\textbf{0.317}   & \\textbf{0.497}\n  \\end{tabular}\n\n\\begin{textblock}{4.2}(11.2,2.4)\n\\begin{block}{}\\fontsize{11}{11}\\sf\n\\begin{itemize}\\tightlist\n\\item ML methods not significantly different.\n\\item MinT methods significantly different from each other and from ML methods.\n\\end{itemize}\\end{block}\n\\end{textblock}\n\n## ML and regularization\n\n\n\n* Replace the linear regression formulation with a less restrictive method to obtain combinations of forecasts from the various hierarchical levels.\n* Coherence is achieved via a bottom-up approach, or by embedding coherence in the ML training.\n*\n\n\n\\textcite{Gle2020} attempts to overcome the lack of focus on coherence by adjusting the objective function. Using neural network forecasts, he includes a regularisation term that penalises incoherences in the generated forecasts. This follows from @mishchenko2019self who proposed a similar regularisation term to obtain reconciled forecasts directly from the base forecasts. The disadvantage of these regularisation approaches is that they result in soft constrains that do not guarantee coherence. \\textcite{Gle2020} provides two alternatives for the regularisation term and shows that the resulting forecasts can outperform standard MinT reconciliation. However, when the regulisation is used in conjunction with MinT this results in both coherent and the most accurate forecasts. \\textcite{HanEtAl2021} propose a similar approach, where a regularisation term is added in the loss function, again based on coherence constraints. They also consider a regularised loss for producing coherent quantile forecasts. The authors demonstrate the use of the proposed regularised loss on a variety of linear and ML models, and also empirically show the negative effect of regularisation on coherence.\n\n\\textcite{ShiEtAl2020} introduces a regularisation term, based on the coherence constraints, in the objective function to push bottom-level forecasts to fit both on their target series and their aggregate counterparts. They forecast the bottom-level series but as the regularisation cannot ensure coherence, these are used in a bottom-up setting to produce coherent forecasts for the rest of the hierarchy. The authors demonstrate the efficacy of this in the context of neural networks. They find that these outperform conventionally trained networks whose forecasts are then reconciled, either with bottom-up or MinT.\n\n\\textcite{ParEtAl2021} propose a regularised neural network with sequence-to-sequence architecture. Focusing on the hierarchical part of the contribution, a regularisation term is added to incorporate the coherence constraints. As with the previous work, this does not guarantee coherence, yet forces the final forecasts to be approximately coherent. The regularisation is embedded in the loss function of the network, achieving an integrated approach. In contrast to the previous work, the network outputs forecasts for all the levels.\n\nThe contribution by \\textcite{AndLi2021} can be seen to belong loosely in the regularisation approaches. Focusing on the M5 competition dataset, the authors produce separate forecasts for the top- and the bottom-levels of the hierarchy, using different methods (NBEATS and LightGBM respectively). To achieve coherent forecasts they modify the loss function of the bottom-level method, where positive errors can are multiplied by a factor. This factor is identified by minimising the incoherence error between the summed bottom-level forecasts and the top-level forecast. Note that this factor is calibrated by keeping the top-level forecasts fixed, and therefore any coherence is obtained by modifying the bottom-level forecasts.\n\n\\textcite{RanEtAl2021} propose another way to achieve an integrated forecast-reconciliation mechanism. First, a global neural network is used to forecast the time series in the hierarchy, which with the assumption of a forecast distribution can produce sample forecasts. The sample forecasts are subsequently reconciled, obtaining distributions of coherent probabilistic forecasts. The calculation of the loss for the training of the model makes use of the reconciled forecasts, allowing an end-to-end approach that parametrises the model to achieve both accurate and coherent forecasts. Note that the methodology allows for various assumptions on the forecast distribution, and the relaxing of these. The resulting forecasts guarantee coherence, in contrast to the previous integrated approaches. The substantive difference with conventional hierarchical approaches, that post-processes base forecasts, is that the global network can model richer interconnections between the time series in the hierarchy for the generation of the forecasts. Furthermore, the authors compare the results of the proposed hierarchical model against a global learner without coherent forecasts and demonstrate on average better performance, suggesting that the integrated methodology offers gains beyond any achieved by the global learning.\n\n\\textcite{WanEtAl2022} contribute with a similar formulation. The important differences are in that they use an autoregressive transformer to produce the base forecasts and that their approach does not require assuming particular predictive distributions. Instead, they rely on empirical estimation (conditional normalising flow) for the distributions. Furthermore, their approach focuses on obtaining bottom-level forecasts which are internally aggregated in a bottom-up fashion to the complete hierarchy. Similar to the \\textcite{RanEtAl2021}, the errors of the final forecasts are used during training, realising an end-to-end hierarchical forecasting method. The authors demonstrate gains in performance over a non-hierarchical version of the their method, as well as over various benchmarks.\n\nFocusing on temporal hierarchies, \\textcite{TheKou2021} provide an end-to-end neural network based method. Similarly to \\textcite{BurChe2021} they explore a series of encoder-decoders to achieve reconciliation, considering fixed and trained decoder weights, using the complete or only the bottom-level of the hierarchy. These can be used instead of conventional hierarchical methods, demonstrating good performance in a global learning setting. To achieve an end-to-end integration, they pass the temporal hierarchy data through a convolutional layer and subsequently to an LSTM. The convolutional layer compresses the abstracts the hierarchical time series, and the LSTM models the dynamics over time. By appending an encoder-decoder to the outputs of the LSTM an end-to-end methodology is obtained. The authors demonstrate the gains due to the various components, but also investigate the amount of time series that are needed to achieve good performance in a global training setting. By modifying the training loss function, \\textcite{TheKou2020} extend the method to provide quantile forecasts.\n\n\\textcite{AboEtAl2022} use ML to address the challenge of selecting the best method to perform the hierarchical reconciliation. To achieve this they rely on a meta-learning classifier, which is trained to identify given the time series features of a dataset what is the best reconciliation method to employ. The approach is quite flexible in terms of features and classifiers, as well as the forecasting models and reconciliation methods.\n\n\\textcite{AboEtAl2019} use ML to improve the performance of non-combination hierarchical approaches. They consider various ML methods to obtain better prorations to decompose upper-level forecasts to lower-levels. \\textcite{FenZha2020} provide a standard implementation of hierarchical methods using base forecasts from ML models, and find the hierarchically reconciled forecasts to be the most accurate. \\textcite{PunEtAl2020} leverage on LSTM networks to produce forecasts for the cross-temporal case. Forecasts are generated for the various temporal aggregation levels, which are reconciled first by using temporal hierarchies and then cross-sectional. \\textcite{SprEtAl2021} propose a method for probabilistic gradient boosting machines, and benchmark its performance on the hierarchical M5 dataset, demonstrating good performance against other non-hierarchical ML methods. Although the proposed method can provide probabilistic predictions for all time series of the hierarchy, coherence is not established. \\textcite{ManEtAl2021} use a deep neural network to directly produce reconciled forecasts. The neural network captures the structure of the hierarchy, as well links the relationship between time series features extracted at any level of the hierarchy and explanatory variables into an end-to-end neural network.\n\n\n\n# In-built coherence\n\n## In-built coherence\n\n**Two-step approach**: compute base forecasts $\\hat{\\bm{y}}_h$, and then reconcile them to produce $\\tilde{\\bm{y}}_h$.\n\n**One-step approaches:** compute coherent $\\tilde{\\bm{y}}_h$ directly.\n\n  * @lhf: linear regression models\n  * @PenvanDal2017: state space models\n  * @villegas2018supply: state space models\n\n## In-built coherence using linear models\n\\fontsize{13}{14}\\sf\nSuppose $\\hat{y}_{t,i} = \\hat{\\bm{\\beta}}_{i}' \\bm{x}_{t,i}$\nwith $\\bm{x}_{t,i}= (1, x_{t,1,i},\\dots,x_{t,p,i})$&nbsp; & &nbsp;\\rlap{$\\hat{\\bm{y}}_i = (\\hat{y}_{1,i}, \\dots, \\hat{y}_{T,i})$.}\n\\pause\n\\begin{align*}\n\\textcolor{blue}{\\begin{pmatrix}\n  \\hat{\\bm{y}}_1\\\\\n  \\hat{\\bm{y}}_2\\\\\n  \\vdots\\\\\n  \\hat{\\bm{y}}_n\n  \\end{pmatrix}} &=\n  \\textcolor{red}{\\begin{pmatrix}\n  \\bm{X}_1 & 0        & \\dots  & 0\\\\\n  0        & \\bm{X}_2 & \\ddots & \\vdots \\\\\n  \\vdots   & \\ddots   & \\ddots & 0\\\\\n  0        & \\dots    & 0      & \\bm{X}_n\n  \\end{pmatrix}}\n  \\begin{pmatrix}\n  \\hat{\\bm{\\beta}}_1\\\\\n  \\hat{\\bm{\\beta}}_2\\\\\n  \\vdots\\\\\n  \\hat{\\bm{\\beta}}_n\n\\end{pmatrix},\n&&\\quad\n  \\bm{X}_i = \\begin{pmatrix}\n  1 & x_{1,i,1} & x_{1,i,2} & \\dots & x_{1,i,p}\\\\\n  1 & x_{2,i,1} & x_{2,i,2} & \\dots & x_{2,i,p}\\\\\n  \\vdots & \\vdots & \\vdots & & \\vdots \\\\\n  1 & x_{T,i,1} & x_{T,i,2} & \\dots & x_{T,i,p}\n\\end{pmatrix}\n\\end{align*}\\pause\n$\\hat{\\bm{B}} = (\\textcolor{red}{\\bm{X}}'\\textcolor{red}{\\bm{X}})^{-1} \\textcolor{red}{\\bm{X}}'\\textcolor{blue}{\\bm{Y}}$\n\\pause\\qquad\n$\\hat{\\bm{y}}_{t+h} = \\bm{X}_{t+h}^* \\hat{\\bm{B}}$\n\\pause\\qquad\n$\\bm{X}^*_{t+h} =\\text{diag}(\\bm{x}_{t+h,i}', \\dots, \\bm{x}_{t+h,n}')$\n\\pause\n\\begin{alertblock}{}\\vspace*{-0.8cm}\n\\begin{align*}\n\\tilde{\\bm{y}}_{t+h} & = \\bm{S}(\\bm{S}'\\bm{W}_h\\bm{S})^{-1}\\bm{S}'\\bm{W}_h\n                            \\hat{\\bm{y}}_{t+h}\n                         = \\bm{S}(\\bm{S}'\\bm{W}_h\\bm{S})^{-1}\\bm{S}'\\bm{W}_h\n                            \\bm{X}_{t+h}^* (\\textcolor{red}{\\bm{X}}'\\textcolor{red}{\\bm{X}})^{-1} \\textcolor{red}{\\bm{X}}'\\textcolor{blue}{\\bm{Y}}\n\\\\\n\\bm{V}_h &= \\sigma^2\\bm{S}(\\bm{S}'\\bm{W}_h\\bm{S})^{-1}\\bm{S}'\\bm{W}_h\\big[1 + \\bm{X}_{T+h}^*(\\textcolor{red}{\\bm{X}}'\\textcolor{red}{\\bm{X}})^{-1}(\\bm{X}_{T+h}^*)'\\big] \\bm{W}_h\\bm{S}'(\\bm{S}'\\bm{W}_h\\bm{S})^{-1}\\bm{S}'\n\\end{align*}\n\\end{alertblock}\n\\only<4->{\\begin{alertblock}{}\nReference: \\citet{lhf}\n\\end{alertblock}}\n\n\\vspace*{10cm}\n\n## In-built coherence\n\n@PenvanDal2017 propose the state space model\n\\begin{align}\\label{eq:measurement}\n\\bm{y}_t &= \\bm{S}\\bm{\\mu}_t + \\bm{Z}_t\\bm{\\beta} + \\bm{\\varepsilon}_t, && \\bm{\\varepsilon}_t \\sim N(\\bm{0}, \\bm{\\Sigma}_{\\bm{\\varepsilon}}),\\\\\n\\bm{\\mu}_t &= \\bm{\\mu}_{t-1} + \\bm{\\eta}_t, && \\bm{\\eta}_t \\sim N(\\bm{0}, \\bm{\\Sigma}_{\\bm{\\eta}}).\n\\end{align}\n\n* Coherent forecasts arise naturally using the Kalman filter\n* Covariance matrices difficult to estimate except for small hierarchies.\n\n## In-build coherence\n\nA related state space approach was proposed by @villegas2018supply, who show that their formulation subsumes bottom-up, top-down, and some forms of forecast reconciliation and combination forecasting.\n\n\n\\nocite{htsgeometry, nonnegmint, Di_FonGir2022b, Van_ErvCug2015, lhf}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}