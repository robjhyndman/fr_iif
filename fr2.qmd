---
title: Forecast reconciliation
subtitle: 2. Perspectives on forecast reconciliation
author: Rob J Hyndman
pdf-engine: pdflatex
fig-width: 9
fig-height: 4.5
format:
  beamer:
    theme: monash
    aspectratio: 169
    fontsize: 14pt
    section-titles: false
    knitr:
      opts_chunk:
        dev: "CairoPDF"
include-in-header: header.tex
cite-method: biblatex
bibliography: hts.bib
biblio-title: References
keep-tex: true
execute:
  echo: false
  message: false
  warning: false
  cache: true
---

```{r}
source("setup.R")
```

## Outline

\vspace*{0.4cm}\tableofcontents

# Reconciliation via constraints

## Notation reminder

\begin{textblock}{8.5}(0.2,1.5)
Every collection of time series with linear constraints can be written as
\centerline{\colorbox[RGB]{210,210,210}{$\bY_{t}=\color{blue}\bS\color{red}\bm{b}_{t}$}}
\vspace*{-0.9cm}\begin{itemize}\parskip=0cm\itemsep=0cm
\item $\by_t=$ vector of all series at time $t$
\item $ y_{\text{Total},t}= $ aggregate of all series at time
$t$.
\item $ y_{X,t}= $ value of series $X$ at time $t$.
\item $\color{red}{\bm{b}_t}=$ vector of most disaggregated series at time $t$
\item $\color{blue}{\bS}=$ ``summing matrix'' containing the linear constraints.
\end{itemize}
\end{textblock}

\begin{textblock}{5.7}(11.4,0.1)
\begin{minipage}{4cm}
\begin{block}{}\centering
\begin{tikzpicture}
\tikzstyle{every node}=[ellipse,draw,fill=red!15,inner sep=2pt]
\tikzstyle[level distance=.3cm]
\tikzstyle[sibling distance=12cm]
\tikzstyle{level 1}=[sibling distance=10mm,font=\small,set style={{every node}+=[fill=blue!15]}]
\node{Total}[edge from parent fork down]
 child {node {A}
 }
 child {node {B}
 }
 child {node {C}
 };
\end{tikzpicture}
\end{block}
\end{minipage}
\end{textblock}

\only<1>{\begin{textblock}{5.7}(9.4,2.8)\fontsize{14}{15}\sf
\begin{align*}
\bY_{t}&= \begin{pmatrix}
  y_{\text{Total},t}\\
  y_{A,t}\\
  y_{B,t}\\
  y_{C,t}
  \end{pmatrix}  \\
  &= {\color{blue}\underbrace{\begin{pmatrix}
                1 & 1 & 1 \\
                1 & 0 & 0 \\
                0 & 1 & 0\\
                0 & 0 & 1
                \end{pmatrix}}_{\bS}}
     {\color{red}\underbrace{\begin{pmatrix}
       y_{A,t}\\y_{B,t}\\y_{C,t}
       \end{pmatrix}}_{\bm{b}_{t}}}
\end{align*}
\end{textblock}}
\only<2>{\begin{textblock}{5.7}(9.4,3.2)\fontsize{14}{15}\sf
\begin{alertblock}{}
\begin{itemize}\itemsep=0.1cm
\item Base forecasts: $\hat{\bm{y}}_{T+h|T}$
\item Reconciled forecasts: $\tilde{\bm{y}}_{T+h|T}=\bS\bm{G}\hat{\bm{y}}_{T+h|T}$
\item MinT: $\bG = (\bS'\bm{W}_h^{-1}\bS)^{-1}\bS'\bm{W}_h^{-1}$ where $\bm{W}_h$ is covariance matrix of base forecast errors.
\end{itemize}
\end{alertblock}
\end{textblock}}

## Notation

\begin{textblock}{5.7}(11.4,0.1)
\begin{minipage}{4cm}
\begin{block}{}\centering
\begin{tikzpicture}
\tikzstyle{every node}=[ellipse,draw,fill=red!15,inner sep=2pt]
\tikzstyle[level distance=.3cm]
\tikzstyle[sibling distance=12cm]
\tikzstyle{level 1}=[sibling distance=10mm,font=\small,set style={{every node}+=[fill=blue!15]}]
\node{Total}[edge from parent fork down]
 child {node {A}
 }
 child {node {B}
 }
 child {node {C}
 };
\end{tikzpicture}
\end{block}
\end{minipage}
\end{textblock}

\begin{textblock}{6}(0.5,1.5)
\begin{block}{Aggregation matrix}\vspace*{-0.6cm}
\begin{align*}
\bY_{t} & =\color{blue}\bS\color{red}\bm{b}_{t} \\[0.3cm]
\begin{pmatrix}
   \textcolor{DarkYellow}{y_{\text{Total},t}}\\
   \textcolor{red}{y_{A,t}}\\
   \textcolor{red}{y_{B,t}}\\
   \textcolor{red}{y_{C,t}}
  \end{pmatrix}
  &= {\color{blue}\begin{pmatrix}
                \textcolor{DarkYellow}1 & \textcolor{DarkYellow}1 & \textcolor{DarkYellow}1 \\
                1 & 0 & 0 \\
                0 & 1 & 0\\
                0 & 0 & 1
                \end{pmatrix}}
     {\color{red}\begin{pmatrix}
       y_{A,t}\\y_{B,t}\\y_{C,t}
       \end{pmatrix}} \\[0.2cm]
  \begin{pmatrix}\textcolor{DarkYellow}{\bm{a}_t}\\\textcolor{red}{\bm{b}_t}\end{pmatrix}
   & = \begin{pmatrix}\textcolor{DarkYellow}{\bm{A}}\\\bm{I}_{n_b}\end{pmatrix}\textcolor{red}{\bm{b}_t}
\end{align*}
\end{block}
\end{textblock}

\begin{textblock}{7}(8.1,3.5)
\begin{block}{Constraint matrix}\vspace*{-0.6cm}
\begin{align*}
\bm{C} \bY_t & = \bm{0} \\
\text{where}\qquad
\bm{C} & =  \begin{bmatrix} 1 & -1 & -1 & -1 \end{bmatrix} \\
  & = \begin{bmatrix} \bm{I}_{n_a} & -\textcolor{DarkYellow}{\bm{A}} \end{bmatrix}
\end{align*}
\end{block}
\end{textblock}


## Zero-constraint representation
\vspace*{0.2cm}

\begin{block}{Aggregation matrix $\bm{A}$}
$$\bm{y}_t
    = \begin{bmatrix}\bm{a}_t\\\bm{b}_t\end{bmatrix}
    = \begin{bmatrix}\bm{A}\\\bm{I}_{n_b}\end{bmatrix}\bm{b}_t
    = \bm{S}\bm{b}_t
$$
\end{block}\pause

\begin{block}{Constraint matrix $\bm{C}$}
\centerline{$\bm{C}\bm{y}_t = \bm{0}$}
\end{block}\vspace*{-0.3cm}

* Constraint matrix approach more general & more parsimonious.
* $\bm{C} = [\bm{I}_{n_a} ~~~ {-\bm{A}}]$.
* $\bm{S}$, $\bm{A}$ and $\bm{C}$ may contain any real values (not just 0s and 1s).

## Zero-constraint representation

Assuming $\bm{C}$ is full rank
\begin{block}{}
\centerline{$\tilde{\bm{y}}_{T+h|T} = \bm{M}\hat{\bm{y}}_{T+h|T}$}
\centerline{where\qquad $\bm{M} = \bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}$}
\end{block}\vspace*{-0.3cm}

* Originally proved by Byron (1978, 1979) for reconciling data.
* Re-discovered by @mint for reconciling forecasts.
* $\bm{M} = \bm{S}\bm{G}$ (the MinT solution)
* Leads to more efficient reconciliation than using $\bm{G}$.

# The geometry of forecast reconciliation

## Projections in linear algebra

\begin{textblock}{9}(.2,1.25)
\begin{tikzpicture}
  % Define coordinates
  \coordinate (O) at (0,0);
  \coordinate (A) at (4,2);
  \coordinate (B) at (2,3);
  \coordinate (L) at (1,5);

  % Draw axes
  \draw[->,thick] (0,0) -- (5,0) node[anchor=north] {$x$};
  \draw[->,thick] (0,0) -- (0,4) node[anchor=east] {$y$};

  % Draw light
  \path [fill = yellow!30!white] (L) circle (.9cm);
  \path [fill = yellow!60!white] (L) circle (.6cm);
  \path [fill = yellow!90!white] (L) circle (.3cm);

  % Draw vectors
  \draw[->,thick,red] (O) -- (A);
  \draw[->,thick,blue] (O) -- (B);

  % Draw projection
  \coordinate (P) at ($(A)!(B)!(O)$);
  \only<2->{\draw[->,thick,blue,dashed] (B) -- (P);}
  \only<2->{\draw[->,thick,blue, dashed] (O) -- (P);}
\end{tikzpicture}
\end{textblock}

```{r}
#| include: false
library(scatterplot3d)
Cairo::CairoPDF(file="figs/scatterplot1.pdf", width=7, height=7)
s3d <- scatterplot3d(trees, type = "p", color = "blue",
    angle=55, pch = 16)
crop::dev.off.crop()
Cairo::CairoPDF(file="figs/scatterplot2.pdf", width=7, height=7)
s3d <- scatterplot3d(trees, type = "h", color = "blue",
    angle=55, pch = 16)
for(i in seq(NROW(trees))) {
  s3d$points3d(trees$Girth[i], trees$Height[i], 10,
    col = "red", type = "p", pch=16, cex=.75)
}
crop::dev.off.crop()
Cairo::CairoPDF(file="figs/scatterplot3.pdf", width=7, height=7)
s3d <- scatterplot3d(trees, type = "p", color = "blue",
    angle=55, pch = 16)
# Add regression plane
my.lm <- lm(trees$Volume ~ trees$Girth + trees$Height)
s3d$plane3d(my.lm, col='red')
crop::dev.off.crop()
Cairo::CairoPDF(file="figs/scatterplot4.pdf", width=7, height=7)
s3d <- scatterplot3d(trees, type = "p", color = "blue",
    angle=55, pch = 16)
# Add regression plane
my.lm <- lm(trees$Volume ~ trees$Girth + trees$Height)
s3d$plane3d(my.lm, col='red')
fits <- fitted(my.lm)
for(i in seq(NROW(trees))) {
  s3d$points3d(
    rep(trees$Girth[i], 2),
    rep(trees$Height[i], 2),
    c(fits[i], trees$Volume[i]),
    col = "blue", type = "l")
  s3d$points3d(
    trees$Girth[i], trees$Height[i], fits[i],
    col = "red", type = "p", cex=.75, pch=16)
}
crop::dev.off.crop()
system("pdfcrop figs/scatterplot1.pdf figs/scatterplot1.pdf")
system("pdfcrop figs/scatterplot2.pdf figs/scatterplot2.pdf")
system("pdfcrop figs/scatterplot3.pdf figs/scatterplot3.pdf")
system("pdfcrop figs/scatterplot4.pdf figs/scatterplot4.pdf")
```

\begin{textblock}{10}(7,1.6)
\only<3>{\includegraphics[width=8.8cm]{scatterplot1}}
\only<4>{\includegraphics[width=8.8cm]{scatterplot2}}
\only<5>{\includegraphics[width=8.8cm]{scatterplot3}}
\only<6>{\includegraphics[width=8.8cm]{scatterplot4}}
\end{textblock}

## Projections in linear algebra

* A projection is a linear transformation $\bm{M}$ such that $\bm{M}^2=\bm{M}$.
* i.e., $M$ is idempotent: it leaves its image unchanged.
* $\bm{M}$ projects onto $\mathfrak{s}$ if $\bm{M}\bm{y}=\bm{y}$ for all $\bm{y}\in\mathfrak{s}$.
* All eigenvalues of $\bm{M}$ are either 0 or 1.
* All singular values of $\bm{M}$ are greater than or equal to 1 (with equality iff $\bm{M}$ is orthogonal).
* A projection is *orthogonal* if $\bm{M}'=\bm{M}$.
* If a projection is not orthogonal, it is called *oblique*.
* In regression, OLS is an orthogonal projection onto space spanned by predictors.

## The coherent subspace

\begin{textblock}{9}(.2,1.25)\fontsize{13}{13}\sf
\begin{block}{Coherent subspace}
$m$-dimensional linear subspace $\mathfrak{s}\subset \mathbb{R}^n$ for which linear constraints hold for all $\bm{y}\in\mathfrak{s}$.
\end{block}\vspace*{-0.3cm}
\begin{block}{Hierarchical time series}
An $n$-dimensional multivariate time series such that $\bm{y}_t\in\mathfrak{s}\quad\forall t$.
\end{block}\vspace*{-0.3cm}
\begin{block}{Coherent point forecasts}
$\tilde{\bm{y}}_{t+h|t}$ is \emph{coherent} if $\tilde{\bm{y}}_{t+h|t} \in \mathfrak{s}$.
\end{block}\vspace*{-0.2cm}
\end{textblock}
\only<2-3>{\begin{textblock}{7.5}(.2,6.75)\fontsize{13}{13}\sf
\begin{alertblock}{Base forecasts}
Let $\hat{\bm{y}}_{t+h|t}$ be vector of \emph{incoherent} initial $h$-step forecasts.$\phantom{y_{t|h}}$
\end{alertblock}
\end{textblock}}
\only<3>{\begin{textblock}{7.5}(8.3,6.75)\fontsize{13}{13}\sf
\begin{alertblock}{Reconciled forecasts}
Let $\psi$ be a mapping, $\psi:\mathbb{R}^n\rightarrow\mathfrak{s}$.  $\tilde{\bm{y}}_{t+h|t}=\psi(\hat{\bm{y}}_{t+h|t})$ ``reconciles'' $\hat{\bm{y}}_{t+h|t}$.
\end{alertblock}
\end{textblock}}

\placefig{9.4}{.0}{width=6.6cm}{3D_hierarchy}
\begin{textblock}{3}(11.4,5.6)\fontsize{13}{13}\sf
\begin{block}{}
\centerline{$ y_{Tot} = y_A + y_B$}
\end{block}
\end{textblock}

## Linear projection reconciliation

\only<1>{\placefig{9.5}{1.5}{width=6.2cm}{InsampDir_2_George}}
\only<2>{\placefig{9.5}{1.5}{width=6.2cm}{OrthProj_George}}
\only<3->{\placefig{9.5}{1.5}{width=6.2cm}{ObliqProj_George}}

\begin{textblock}{9}(.5,1.5)
  \begin{itemize}\tightlist
  \item $R$ is the most likely direction of deviations from $\mathfrak{s}$.
  \only<1->{\item Grey: potential base forecasts}
  \only<2->{\item Red: reconciled forecasts}
  \only<2->{\item Orthogonal projections (i.e., OLS) lead to smallest possible adjustments of base forecasts.}
  \only<3->{\item Oblique projections (i.e., MinT) give reconciled forecasts with smallest variance.}
  \end{itemize}
\end{textblock}

\only<2->{
  \begin{textblock}{4.5}(11.2,7.6)\fontsize{13}{14}\sf
  \begin{block}{}
  \only<2>{Orthogonal projection}
  \only<3>{Oblique projection}
  \end{block}
  \end{textblock}
}

## Linear projection reconciliation
\fontsize{14}{16}\sf
\vspace*{0.2cm}\begin{alertblock}{}
\centerline{$\tilde{\bm{y}}_{t+h|t}= \psi(\hat{\bm{y}}_{t+h|t}) = \bm{M}\hat{\bm{y}}_{t+h|t}$}
\end{alertblock}\vspace*{-0.5cm}

* $\bm{M}$ is a projection onto $\mathfrak{s}$ if and only if $\bm{M}\bm{y}=\bm{y}$ for all $\bm{y}\in\mathfrak{s}$.
* Coherent base forecasts are unchanged since $\bm{M}\hat{\bm{y}}=\hat{\bm{y}}$
* If $\hat{\bm{y}}$ is unbiased, then $\tilde{\bm{y}}$ is also unbiased since
$$
  \E(\tilde{\bm{y}}_{t+h|t}) = \E(\bm{M}\hat{\bm{y}}_{t+h|t}) = \bm{M} \E(\hat{\bm{y}}_{t+h|t}) = \E(\hat{\bm{y}}_{t+h|t}),
$$
and unbiased estimates must lie on $\mathfrak{s}$.
* The projection is orthogonal if and only if $\bm{M}'=\bm{M}$.
* $\bm{S}$ forms a basis set for $\mathfrak{s}$.
* Projections are of the form $\bm{M} = \bS(\bS'\bm{\Psi}\bS)^{-1}\bS'\bm{\Psi}$ where $\bm{\Psi}$ is a positive definite matrix.

\vspace*{10cm}

## Linear projection reconciliation

\vspace*{0.2cm}\begin{alertblock}{}
\centerline{$\tilde{\bm{y}}_{t+h|t}= \psi(\hat{\bm{y}}_{t+h|t}) = \bm{M}\hat{\bm{y}}_{t+h|t},\qquad\text{where}\quad \bm{M} = \bS(\bS'\bm{\Psi}\bS)^{-1}\bS'\bm{\Psi}$}
\end{alertblock}\vspace*{.01cm}
\begin{block}{}\vspace*{-0.6cm}
\begin{align*}
&\text{OLS:}  && \bm{\Psi}=\bm{I} &&& \bm{M} & = \bm{S}(\bm{S}'\bm{S})^{-1}\bm{S}' && = \bm{I} - \bm{C}'(\bm{C}\bm{C}')^{-1}\bm{C} \\
&\text{MinT:} && \bm{\Psi}=\bm{W}_h &&&\bm{M} & = \bS(\bS'\bm{W}_h^{-1}\bS)^{-1}\bS'\bm{W}_h^{-1} && = \bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}
\end{align*}
\end{block}\vspace*{-0.3cm}

* $\bm{M}$ is orthogonal iff $\bm{\Psi}=\bm{I}$.
* $\bm{W}_h = \var[\by_{T+h} - \hat{\by}_{T+h|T} \mid \by_1,\dots,\by_T]$ is the covariance matrix of the base forecast errors.
* $\bm{V}_h = \var[\by_{T+h} - \tilde{\by}_{T+h|T}  \mid \by_1,\dots,\by_T]  = \bm{M}\bm{W}_h\bm{M}'$ is minimized when  $\bm{\Psi} = \bm{W}_h$.

\vspace*{10cm}

# Mean square error bounds

## Mean square error bounds

\vspace*{0.2cm}\begin{block}{Distance reducing property}
Let $\|\bm{u}\|_{\bm{\Psi}} = \bm{u}'\bm{\Psi}\bm{u}$. Then
  \centerline{$\|\bm{y}_{t+h}-\tilde{\bm{y}}_{t+h|t}\|_{\bm{\Psi}}\le\|\bm{y}_{t+h}-\hat{\bm{y}}_{t+h|t}\|_{\bm{\Psi}}$}
\end{block}

 * $\bm{\Psi}$-projection is guaranteed to improve forecast accuracy over base forecasts *using this distance measure*.
 * Distance reduction holds for any realisation and any forecast.
 * OLS reconciliation minimizes Euclidean distance.
 * Other measures of forecast accuracy may be worse.

## Mean square error bounds

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{wickramasuriya2021properties}
\end{block}\end{textblock}

\begin{block}{}\vspace*{-0.6cm}
\begin{align*}
\|\bm{y}_{t+h} - \tilde{\bm{y}}_{t+h}\|_2^2
 &= \|\bm{M}(\bm{y}_{t+h} - \hat{\bm{y}}_{t+h})\|_2^2 \\
 &\le \|\bm{M}\|_2^2 \|\bm{y}_{t+h} - \hat{\bm{y}}_{t+h}\|_2^2 \\
 & = \sigma_{\text{max}}^2\|\bm{y}_{t+h} - \hat{\bm{y}}_{t+h}\|_2^2
\end{align*}
\end{block}

 * $\sigma_{\text{max}}$ is the largest eigenvalue of $\bm{M}$
 * $\sigma_{\text{max}}\ge1$ as $\bm{M}$ is a projection matrix.
 * Every projection reconciliation is better than base forecasts using Euclidean distance.

## Mean square error bounds

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{wickramasuriya2021properties}
\end{block}\end{textblock}
\vspace*{0.2cm}\begin{block}{}\vspace*{-0.6cm}
\begin{align*}
    & \text{tr}\Big(\E[\bm{y}_{t+h} - \tilde{\bm{y}}^{\text{MinT}}_{t+h|t}]'[\bm{y}_{t+h} - \tilde{\bm{y}}^{\text{MinT}}_{t+h|t}]\Big) \\
\le~ & \text{tr}\Big(\E[\bm{y}_{t+h} - \tilde{\bm{y}}^{\text{OLS}}_{t+h|t}]'[\bm{y}_{t+h} - \tilde{\bm{y}}^{\text{OLS}}_{t+h|t}]\Big) \\
\le~ & \text{tr}\Big(\E[\bm{y}_{t+h} - \hat{\bm{y}}_{t+h|t}]'[\bm{y}_{t+h} - \hat{\bm{y}}_{t+h|t}]\Big)
\end{align*}
\end{block}

Using sums of variances:

* MinT reconciliation is better than OLS reconciliation
* OLS reconciliation is better than base forecasts

# Other optimization approaches

## A game theoretic perspective
\fontsize{14}{16}\sf

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{Van_ErvCug2015}
\end{block}\end{textblock}

Find the solution to the minimax problem
$$
V = \mathop{min}_{\tilde{\bm{y}} \in \mathfrak{s}}
\mathop{max}_{\bm{y}\in \mathfrak{s}}
\left\{\ell(\bm{y},\tilde{\bm{y}}) -
\ell(\bm{y},\hat{\bm{y}})
\right\},
$$
where $\ell$ is a loss function, and $\mathfrak{s}$ is the coherent subspace.

* $V\le0$: reconciliation guaranteed to reduce loss.
* If $\ell(\bm{y},\tilde{\bm{y}}) = \|\bm{y}- \tilde{\bm{y}}\|_{\bm{\Psi}} = (\bm{y}-\tilde{\bm{y}})'\bm{\Psi}(\bm{y}-\tilde{\bm{y}})$, where $\bm{\Psi}$ is any symmetric pd matrix, then:

  1. $\tilde{\bm{y}}=\bm{S}(\bm{S}'\bm{\Psi}\bm{S})^{-1}\bm{S}'\bm{\Psi}\hat{\bm{y}}$ will always improve upon the base forecasts;
  2. The MinT solution $\tilde{\bm{y}}=\bm{S}(\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}\bm{S}'\bm{W}_h^{-1}\hat{\bm{y}}$ will optimise loss in expectation over any choice of $\bm{\Psi}$.

## Biased reconciliation

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{Ben_TaiEtAl2019}
\end{block}\end{textblock}

Regularized empirical risk minimization problem:
$$\min_{\bm{G}} \frac{1}{Nn}\|\bm{Y} - \hat{\bm{Y}}\bm{G}'\bm{S}'\|_F + \lambda \|\text{vec}{\bm{G}}\|_1,
$$

* $N=T-T_1-h+1$,\quad $T_1$ is minimum training sample size
* $\|\cdot\|_F$ is the Frobenius norm
* $\bm{Y} = [\bm{y}_{T_1+h}, \dots, \bm{y}_{T}]'$
* $\hat{\bm{Y}} = [\hat{\bm{y}}_{T_1+h|T_1}, \dots, \hat{\bm{y}}_{T|T-h}]'$
* $\lambda$ is a regularization parameter.

When $\lambda=0$:\qquad  $\hat{\bm{G}} = \bm{B}'\hat{\bm{Y}}(\hat{\bm{Y}}'\hat{\bm{Y}})^{-1}$
where $\bm{B} = [\bm{b}_{T_1+h}, \dots, \bm{b}_{T}]'$.

Reference: @Ben_TaiEtAl2019

## Unconstrained MinT

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{wickramasuriya2021properties}
\end{block}\end{textblock}

Include?


# Adding optimization constraints

## Non-negative forecasts

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{nonnegmint}
\end{block}\end{textblock}

* How to ensure all forecasts are positive?

\pause\begin{block}{}
\centerline{$\min_{\bm{G}_h}\text{tr}\Big(\E[\bm{y}_{t+h} - \bm{S}\bm{G}_h\hat{\bm{y}}_{t+h|t}]'[\bm{y}_{t+h} - \bm{S}\bm{G}_h\hat{\bm{y}}_{t+h|t}]\Big)$}
\centerline{such that $\bm{S}\bm{G}_h\hat{\bm{y}}_{t+h|t} \ge 0$}
\end{block}

with @WicEtAl2020 providing an early example for forecast reconciliation, and @di2023spatio a more recent example.

@di2023spatio also discuss an effective nonnegative heuristic called "set-negative-to-zero", whereby the negative reconciled forecasts at the bottom level are set to zero, and the remaining forecasts computed via aggregation.

## Immutable forecasts

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{HolEtAl2021}
\end{block}\end{textblock}

* How to ensure some forecasts are unchanged?

\pause\begin{block}{}
\centerline{$\min_{\bm{G}_h}\text{tr}\Big(\E[\bm{y}_{t+h} - \bm{S}\bm{G}_h\hat{\bm{y}}_{t+h|t}]'[\bm{y}_{t+h} - \bm{S}\bm{G}_h\hat{\bm{y}}_{t+h|t}]\Big)$}
\centerline{such that $\bm{C}\bm{S}\bm{G}_h\hat{\bm{y}}_{t+h|t} = \bm{d}$}
\end{block}

* Differs from top-down approaches in that it can be done while also preserving the unbiasedness of base forecasts.

* To briefly illustrate the main idea, for a three variable hierarchy where $y_{Tot,t}=y_{A,t}+y_{B,t}$, either setting
$$
  \begin{pmatrix}\tilde{y}_{Tot,t}\\\tilde{y}_{A,t}\\\tilde{y}_{B,t}\end{pmatrix} =
  \begin{pmatrix}\hat{y}_{Tot,t}\\\hat{y}_{A,t}\\\hat{y}_{Tot,t}-\hat{y}_{A,t}\end{pmatrix}
  \qquad\textrm{or}\qquad
  \begin{pmatrix}\tilde{y}_{Tot,t}\\\tilde{y}_{A,t}\\\tilde{y}_{B,t}\end{pmatrix} =
  \begin{pmatrix}\hat{y}_{Tot,t}\\\hat{y}_{Tot,t}-\hat{y}_{B,t}\\\hat{y}_{B,t}\end{pmatrix}
$$

* Average between these two solutions will have the same properties.

* @HolEtAl2021 generalise this idea to more complex hierarchies, and the properties of their methods are investigated by @Di_FonGir2022b. @ZhaEtAl2022 further generalise this idea to a setting where reconciliation can be carried out while keeping a subset of base forecasts unchanged and not just the top level. Conditions on how a set of such "immutable" series can be selected are also provided by @ZhaEtAl2022.

# ML and regularization
## ML and regularization

# Bayesian versions
## Bayesian versions

# In-built coherence

## In-built coherence

\nocite{htsgeometry, nonnegmint, Di_FonGir2022b,swissexports,Van_ErvCug2015}
