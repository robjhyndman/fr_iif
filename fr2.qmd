---
title: Forecast reconciliation
subtitle: 2. Perspectives on forecast reconciliation
author: Rob J Hyndman
pdf-engine: pdflatex
fig-width: 9
fig-height: 4.5
format:
  beamer:
    theme: monash
    aspectratio: 169
    fontsize: 14pt
    section-titles: false
    knitr:
      opts_chunk:
        dev: "CairoPDF"
include-in-header: header.tex
cite-method: biblatex
bibliography: hts.bib
biblio-title: References
keep-tex: true
execute:
  echo: false
  message: false
  warning: false
  cache: true
---

```{r}
source("setup.R")
```

## Outline

\vspace*{0.4cm}\tableofcontents

# Reconciliation via constraints

## Notation reminder

\begin{textblock}{8.5}(0.2,1.5)
Every collection of time series with linear constraints can be written as
\centerline{\colorbox[RGB]{210,210,210}{$\bY_{t}=\color{blue}\bS\color{red}\bm{b}_{t}$}}
\vspace*{-0.9cm}\begin{itemize}\parskip=0cm\itemsep=0cm
\item $\by_t=$ vector of all series at time $t$
\item $ y_{\text{Total},t}= $ aggregate of all series at time
$t$.
\item $ y_{X,t}= $ value of series $X$ at time $t$.
\item $\color{red}{\bm{b}_t}=$ vector of most disaggregated series at time $t$
\item $\color{blue}{\bS}=$ ``summing matrix'' containing the linear constraints.
\end{itemize}
\end{textblock}

\begin{textblock}{5.7}(11.4,0.1)
\begin{minipage}{4cm}
\begin{block}{}\centering
\begin{tikzpicture}
\tikzstyle{every node}=[ellipse,draw,fill=red!15,inner sep=2pt]
\tikzstyle[level distance=.3cm]
\tikzstyle[sibling distance=12cm]
\tikzstyle{level 1}=[sibling distance=10mm,font=\small,set style={{every node}+=[fill=blue!15]}]
\node{Total}[edge from parent fork down]
 child {node {A}
 }
 child {node {B}
 }
 child {node {C}
 };
\end{tikzpicture}
\end{block}
\end{minipage}
\end{textblock}

\only<1>{\begin{textblock}{5.7}(9.4,2.8)\fontsize{14}{15}\sf
\begin{align*}
\bY_{t}&= \begin{pmatrix}
  y_{\text{Total},t}\\
  y_{A,t}\\
  y_{B,t}\\
  y_{C,t}
  \end{pmatrix}  \\
  &= {\color{blue}\underbrace{\begin{pmatrix}
                1 & 1 & 1 \\
                1 & 0 & 0 \\
                0 & 1 & 0\\
                0 & 0 & 1
                \end{pmatrix}}_{\bS}}
     {\color{red}\underbrace{\begin{pmatrix}
       y_{A,t}\\y_{B,t}\\y_{C,t}
       \end{pmatrix}}_{\bm{b}_{t}}}
\end{align*}
\end{textblock}}
\only<2>{\begin{textblock}{5.7}(9.4,3.2)\fontsize{14}{15}\sf
\begin{alertblock}{}
\begin{itemize}\itemsep=0.1cm
\item Base forecasts: $\hat{\bm{y}}_{T+h|T}$
\item Reconciled forecasts: $\tilde{\bm{y}}_{T+h|T}=\bS\bm{G}\hat{\bm{y}}_{T+h|T}$
\item MinT: $\bG = (\bS'\bm{W}_h^{-1}\bS)^{-1}\bS'\bm{W}_h^{-1}$ where $\bm{W}_h$ is covariance matrix of base forecast errors.
\end{itemize}
\end{alertblock}
\end{textblock}}

## Notation

\begin{textblock}{5.7}(11.4,0.1)
\begin{minipage}{4cm}
\begin{block}{}\centering
\begin{tikzpicture}
\tikzstyle{every node}=[ellipse,draw,fill=red!15,inner sep=2pt]
\tikzstyle[level distance=.3cm]
\tikzstyle[sibling distance=12cm]
\tikzstyle{level 1}=[sibling distance=10mm,font=\small,set style={{every node}+=[fill=blue!15]}]
\node{Total}[edge from parent fork down]
 child {node {A}
 }
 child {node {B}
 }
 child {node {C}
 };
\end{tikzpicture}
\end{block}
\end{minipage}
\end{textblock}

\begin{textblock}{6}(0.5,1.5)
\begin{block}{Aggregation matrix}\vspace*{-0.6cm}
\begin{align*}
\bY_{t} & =\color{blue}\bS\color{red}\bm{b}_{t} \\[0.3cm]
\begin{pmatrix}
   \textcolor{DarkYellow}{y_{\text{Total},t}}\\
   \textcolor{red}{y_{A,t}}\\
   \textcolor{red}{y_{B,t}}\\
   \textcolor{red}{y_{C,t}}
  \end{pmatrix}
  &= {\color{blue}\begin{pmatrix}
                \textcolor{DarkYellow}1 & \textcolor{DarkYellow}1 & \textcolor{DarkYellow}1 \\
                1 & 0 & 0 \\
                0 & 1 & 0\\
                0 & 0 & 1
                \end{pmatrix}}
     {\color{red}\begin{pmatrix}
       y_{A,t}\\y_{B,t}\\y_{C,t}
       \end{pmatrix}} \\[0.2cm]
  \begin{pmatrix}\textcolor{DarkYellow}{\bm{a}_t}\\\textcolor{red}{\bm{b}_t}\end{pmatrix}
   & = \begin{pmatrix}\textcolor{DarkYellow}{\bm{A}}\\\bm{I}_{n_b}\end{pmatrix}\textcolor{red}{\bm{b}_t}
\end{align*}
\end{block}
\end{textblock}

\begin{textblock}{7}(8.1,3.5)
\begin{block}{Constraint matrix}\vspace*{-0.6cm}
\begin{align*}
\bm{C} \bY_t & = \bm{0} \\
\text{where}\qquad
\bm{C} & =  \begin{bmatrix} 1 & -1 & -1 & -1 \end{bmatrix} \\
  & = \begin{bmatrix} \bm{I}_{n_a} & -\textcolor{DarkYellow}{\bm{A}} \end{bmatrix}
\end{align*}
\end{block}
\end{textblock}


## Zero-constraint representation
\vspace*{0.2cm}

\begin{block}{Aggregation matrix $\bm{A}$}
$$\bm{y}_t
    = \begin{bmatrix}\bm{a}_t\\\bm{b}_t\end{bmatrix}
    = \begin{bmatrix}\bm{A}\\\bm{I}_{n_b}\end{bmatrix}\bm{b}_t
    = \bm{S}\bm{b}_t
$$
\end{block}\pause

\begin{block}{Constraint matrix $\bm{C}$}
\centerline{$\bm{C}\bm{y}_t = \bm{0}$}
\end{block}\vspace*{-0.3cm}

* Constraint matrix approach more general & more parsimonious.
* $\bm{C} = [\bm{I}_{n_a} ~~~ {-\bm{A}}]$.
* $\bm{S}$, $\bm{A}$ and $\bm{C}$ may contain any real values (not just 0s and 1s).

## Zero-constraint representation

Assuming $\bm{C}$ is full rank
\begin{block}{}
\centerline{$\tilde{\bm{y}}_{T+h|T} = \bm{M}\hat{\bm{y}}_{T+h|T}$}
\centerline{where\qquad $\bm{M} = \bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}$}
\end{block}\vspace*{-0.3cm}

* Originally proved by Byron (1978, 1979) for reconciling data.
* Re-discovered by @mint for reconciling forecasts.
* $\bm{M} = \bm{S}\bm{G}$ (the MinT solution)
* Leads to more efficient reconciliation than using $\bm{G}$.

# The geometry of forecast reconciliation

## Projections in linear algebra

\begin{textblock}{9}(.2,1.25)
\begin{tikzpicture}
  % Define coordinates
  \coordinate (O) at (0,0);
  \coordinate (A) at (4,2);
  \coordinate (B) at (2,3);
  \coordinate (L) at (1,5);

  % Draw axes
  \draw[->,thick] (0,0) -- (5,0) node[anchor=north] {$x$};
  \draw[->,thick] (0,0) -- (0,4) node[anchor=east] {$y$};

  % Draw light
  \path [fill = yellow!30!white] (L) circle (.9cm);
  \path [fill = yellow!60!white] (L) circle (.6cm);
  \path [fill = yellow!90!white] (L) circle (.3cm);

  % Draw vectors
  \draw[->,thick,red] (O) -- (A);
  \draw[->,thick,blue] (O) -- (B);

  % Draw projection
  \coordinate (P) at ($(A)!(B)!(O)$);
  \only<2->{\draw[->,thick,blue,dashed] (B) -- (P);}
  \only<2->{\draw[->,thick,blue, dashed] (O) -- (P);}
\end{tikzpicture}
\end{textblock}

```{r}
#| include: false
library(scatterplot3d)
Cairo::CairoPDF(file="figs/scatterplot1.pdf", width=7, height=7)
s3d <- scatterplot3d(trees, type = "p", color = "blue",
    angle=55, pch = 16)
crop::dev.off.crop()
Cairo::CairoPDF(file="figs/scatterplot2.pdf", width=7, height=7)
s3d <- scatterplot3d(trees, type = "h", color = "blue",
    angle=55, pch = 16)
for(i in seq(NROW(trees))) {
  s3d$points3d(trees$Girth[i], trees$Height[i], 10,
    col = "red", type = "p", pch=16, cex=.75)
}
crop::dev.off.crop()
Cairo::CairoPDF(file="figs/scatterplot3.pdf", width=7, height=7)
s3d <- scatterplot3d(trees, type = "p", color = "blue",
    angle=55, pch = 16)
# Add regression plane
my.lm <- lm(trees$Volume ~ trees$Girth + trees$Height)
s3d$plane3d(my.lm, col='red')
crop::dev.off.crop()
Cairo::CairoPDF(file="figs/scatterplot4.pdf", width=7, height=7)
s3d <- scatterplot3d(trees, type = "p", color = "blue",
    angle=55, pch = 16)
# Add regression plane
my.lm <- lm(trees$Volume ~ trees$Girth + trees$Height)
s3d$plane3d(my.lm, col='red')
fits <- fitted(my.lm)
for(i in seq(NROW(trees))) {
  s3d$points3d(
    rep(trees$Girth[i], 2),
    rep(trees$Height[i], 2),
    c(fits[i], trees$Volume[i]),
    col = "blue", type = "l")
  s3d$points3d(
    trees$Girth[i], trees$Height[i], fits[i],
    col = "red", type = "p", cex=.75, pch=16)
}
crop::dev.off.crop()
system("pdfcrop figs/scatterplot1.pdf figs/scatterplot1.pdf")
system("pdfcrop figs/scatterplot2.pdf figs/scatterplot2.pdf")
system("pdfcrop figs/scatterplot3.pdf figs/scatterplot3.pdf")
system("pdfcrop figs/scatterplot4.pdf figs/scatterplot4.pdf")
```

\begin{textblock}{10}(7,1.6)
\only<3>{\includegraphics[width=8.8cm]{scatterplot1}}
\only<4>{\includegraphics[width=8.8cm]{scatterplot2}}
\only<5>{\includegraphics[width=8.8cm]{scatterplot3}}
\only<6>{\includegraphics[width=8.8cm]{scatterplot4}}
\end{textblock}

## Projections in linear algebra

* A projection is a linear transformation $\bm{M}$ such that $\bm{M}^2=\bm{M}$.

* i.e., $M$ is idempotent: it leaves its image unchanged.

* $\bm{M}$ projects onto $\mathfrak{s}$ if $\bm{M}\bm{y}=\bm{y}$ for all $\bm{y}\in\mathfrak{s}$.

* A projection is *orthogonal* if $\bm{M}'=\bm{M}$.

* If a projection is not orthogonal, it is called *oblique*.

* In regression, OLS is an orthogonal projection onto space spanned by predictors.


## The coherent subspace

\begin{textblock}{9}(.2,1.25)\fontsize{13}{13}\sf
\begin{block}{Coherent subspace}
$m$-dimensional linear subspace $\mathfrak{s}\subset \mathbb{R}^n$ for which linear constraints hold for all $\bm{y}\in\mathfrak{s}$.
\end{block}\vspace*{-0.3cm}
\begin{block}{Hierarchical time series}
An $n$-dimensional multivariate time series such that $\bm{y}_t\in\mathfrak{s}\quad\forall t$.
\end{block}\vspace*{-0.3cm}
\begin{block}{Coherent point forecasts}
$\tilde{\bm{y}}_{t+h|t}$ is \emph{coherent} if $\tilde{\bm{y}}_{t+h|t} \in \mathfrak{s}$.
\end{block}\vspace*{-0.2cm}
\end{textblock}
\only<2-3>{\begin{textblock}{7.5}(.2,6.75)\fontsize{13}{13}\sf
\begin{alertblock}{Base forecasts}
Let $\hat{\bm{y}}_{t+h|t}$ be vector of \emph{incoherent} initial $h$-step forecasts.$\phantom{y_{t|h}}$
\end{alertblock}
\end{textblock}}
\only<3>{\begin{textblock}{7.5}(8.3,6.75)\fontsize{13}{13}\sf
\begin{alertblock}{Reconciled forecasts}
Let $\psi$ be a mapping, $\psi:\mathbb{R}^n\rightarrow\mathfrak{s}$.  $\tilde{\bm{y}}_{t+h|t}=\psi(\hat{\bm{y}}_{t+h|t})$ ``reconciles'' $\hat{\bm{y}}_{t+h|t}$.
\end{alertblock}
\end{textblock}}

\placefig{9.4}{.0}{width=6.6cm}{3D_hierarchy}
\begin{textblock}{3}(11.4,5.6)\fontsize{13}{13}\sf
\begin{block}{}
\centerline{$ y_{Tot} = y_A + y_B$}
\end{block}
\end{textblock}

## Linear projection reconciliation

\only<1>{\placefig{9.5}{1.5}{width=6.2cm}{InsampDir_2_George}}
\only<2>{\placefig{9.5}{1.5}{width=6.2cm}{OrthProj_George}}
\only<3->{\placefig{9.5}{1.5}{width=6.2cm}{ObliqProj_George}}

\begin{textblock}{9}(.5,1.5)
  \begin{itemize}\tightlist
  \item $R$ is the most likely direction of deviations from $\mathfrak{s}$.
  \only<1->{\item Grey: potential base forecasts}
  \only<2->{\item Red: reconciled forecasts}
  \only<2->{\item Orthogonal projections (i.e., OLS) lead to smallest possible adjustments of base forecasts.}
  \only<3->{\item Oblique projections (i.e., MinT) give reconciled forecasts with smallest variance.}
  \end{itemize}
\end{textblock}

\only<2->{
  \begin{textblock}{4.5}(11.2,7.6)\fontsize{13}{14}\sf
  \begin{block}{}
  \only<2>{Orthogonal projection}
  \only<3>{Oblique projection}
  \end{block}
  \end{textblock}
}

## Linear projection reconciliation
\fontsize{14}{16}\sf
\vspace*{0.2cm}\begin{alertblock}{}
\centerline{$\tilde{\bm{y}}_{t+h|t}= \psi(\hat{\bm{y}}_{t+h|t}) = \bm{M}\hat{\bm{y}}_{t+h|t}$}
\end{alertblock}\vspace*{-0.5cm}

* $\bm{M}$ is a projection onto $\mathfrak{s}$ if and only if $\bm{M}\bm{y}=\bm{y}$ for all $\bm{y}\in\mathfrak{s}$.
* Coherent base forecasts are unchanged since $\bm{M}\hat{\bm{y}}=\hat{\bm{y}}$
* If $\hat{\bm{y}}$ is unbiased, then $\tilde{\bm{y}}$ is also unbiased since
$$
  \E(\tilde{\bm{y}}_{t+h|t}) = \E(\bm{M}\hat{\bm{y}}_{t+h|t}) = \bm{M} \E(\hat{\bm{y}}_{t+h|t}) = \E(\hat{\bm{y}}_{t+h|t}),
$$
and unbiased estimates must lie on $\mathfrak{s}$.
* The projection is orthogonal if and only if $\bm{M}'=\bm{M}$.
* $\bm{S}$ forms a basis set for $\mathfrak{s}$.
* Projections are of the form $\bm{M} = \bS(\bS'\bm{\Psi}\bS)^{-1}\bS'\bm{\Psi}$ where $\bm{\Psi}$ is a positive definite matrix.

\vspace*{10cm}

## Linear projection reconciliation

\vspace*{0.2cm}\begin{alertblock}{}
\centerline{$\tilde{\bm{y}}_{t+h|t}= \psi(\hat{\bm{y}}_{t+h|t}) = \bm{M}\hat{\bm{y}}_{t+h|t},\qquad\text{where}\quad \bm{M} = \bS(\bS'\bm{\Psi}\bS)^{-1}\bS'\bm{\Psi}$}
\end{alertblock}\vspace*{-0.99cm}

\begin{align*}
\text{OLS:}  && \bm{M} & = \bm{S}(\bm{S}'\bm{S})^{-1}\bm{S}' && = \bm{I} - \bm{C}'(\bm{C}\bm{C}')^{-1}\bm{C} \\
\text{MinT:} && \bm{M} & = \bS(\bS'\bm{W}_h^{-1}\bS)^{-1}\bS'\bm{W}_h^{-1} && = \bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}
\end{align*}

* $\bm{\Psi}$ is any positive definite matrix.
* $\bm{W}_h = \var[\by_{T+h} - \hat{\by}_{T+h|T} \mid \by_1,\dots,\by_n]$ is the covariance matrix of the base forecast errors.
* $\bm{M}$ is orthogonal iff $\bm{\Psi}=\bm{I}$.

\vspace*{10cm}

## Linear projection reconciliation

\vspace*{0.2cm}\begin{alertblock}{}
\centerline{$\tilde{\bm{y}}_{t+h|t}= \psi(\hat{\bm{y}}_{t+h|t}) = \bm{M}\hat{\bm{y}}_{t+h|t},\qquad\text{where}\quad \bm{M} = \bS(\bS'\bm{\Psi}\bS)^{-1}\bS'\bm{\Psi}$}
\end{alertblock}

\begin{block}{Variance}
\centerline{$\bm{V}_h = \var[\by_{T+h} - \tilde{\by}_{T+h|T}  \mid \by_1,\dots,\by_n]  = \bm{M}\bm{\Psi}\bm{M}'$}
\end{block}

\begin{alertblock}{Minimum trace (MinT) reconciliation}
If $\bm{M}$ is a projection, then the trace of $\bm{V}_h$ is minimized when $\bm{\Psi} = \bm{W}_h$ (MinT).
\end{alertblock}

\vspace*{10cm}


# Mean square error bounds

## Mean square error bounds

MinT forecasts compared to Base forecasts:
\begin{align*}
\|\bm{y}_{t+h}-\tilde{\bm{y}}_{t+h|t}\|\le\|\bm{y}_{t+h}-\hat{\bm{y}}_{t+h|t}\|
\end{align*}

\begin{align*}
(\bm{y}_{t+h}-\tilde{\bm{y}}_{t+h|t})'\bm{W}_h^{-1}(\bm{y}_{t+h}-\tilde{\bm{y}}_{t+h|t})
\le
(\bm{y}_{t+h}-\hat{\bm{y}}_{t+h|t})'\bm{W}_h^{-1}(\bm{y}_{t+h}-\hat{\bm{y}}_{t+h|t})
\end{align*}

\only<4>{
  \begin{textblock}{9}(0.2,6)
  \begin{block}{Distance reducing property}
  If $\bm{M}$ is an orthogonal projection onto $\mathfrak{s}$:
  \centerline{$\|\bm{y}_{t+h}-\tilde{\bm{y}}_{t+h|t}\|\le\|\bm{y}_{t+h}-\hat{\bm{y}}_{t+h|t}\|$}
  \end{block}
  \end{textblock}
}

\only<4>{
  \begin{textblock}{6.6}(9.2,5.3)\fontsize{12}{12}\sf
  \begin{itemize}\tightlist
  \item Distance reduction holds for any realisation and any forecast.
  \item Other measures of forecast accuracy may be worse.
  \item Not necessarily the optimal reconciliation.
  \end{itemize}
  \end{textblock}
}

## Mean square error bounds

\begin{align*}
\|\bm{y}_{t+h} - \tilde{\bm{y}_{t+h}}\|_2^2
 &= \|\bm{S}\bm{G}_h(\bm{y}_{t+h} - \tilde{\hat{y}_{t+h}})\|_2^2 \\
 &\le \|\bm{S}\bm{G}_h\|_2^2 \|(\bm{y}_{t+h} - \tilde{\hat{y}_{t+h}})\|_2^2 \\
 & = \sigma_{\text{max}}^2\|(\bm{y}_{t+h} - \tilde{\hat{y}_{t+h}})\|_2^2
\end{align*}

 * $\sigma_{\text{max}}^2$ is the largest singular value of $\bm{S}\bm{G}_h$
 * $\sigma_{\text{max}}\ge1$ as $\bm{S}\bm{G}_h$ is a projection matrix.



# Other optimization approaches

## A game theoretic perspective
\fontsize{14}{16}\sf

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{Van_ErvCug2015}
\end{block}\end{textblock}

Find the solution to the minimax problem
$$
V = \mathop{min}_{\tilde{\bm{y}} \in \mathfrak{s}}
\mathop{max}_{\bm{y}\in \mathfrak{s}}
\left\{\ell(\bm{y},\tilde{\bm{y}}) -
\ell(\bm{y},\hat{\bm{y}})
\right\},
$$
where $\ell$ is a loss function, and $\mathfrak{s}$ is the coherent subspace.

* $V\le0$: reconciliation guaranteed to reduce loss.
* If $\ell(\bm{y},\tilde{\bm{y}}) = (\bm{y}-\tilde{\bm{y}})'\bm{\Psi}(\bm{y}-\tilde{\bm{y}})$, where $\bm{\Psi}$ is any symmetric pd matrix, then:

  1. $\tilde{\bm{y}}=\bm{S}(\bm{S}'\bm{\Psi}\bm{S})^{-1}\bm{S}'\bm{\Psi}\hat{\bm{y}}$ will always improve upon the base forecasts;
  2. The MinT solution $\tilde{\bm{y}}=\bm{S}(\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}\bm{S}'\bm{W}_h^{-1}\hat{\bm{y}}$ will optimise loss in expectation over any choice of $\bm{\Psi}$.

Estimation uncertainty may violate this result.

## Biased reconciliation

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{Ben_TaiEtAl2019}
\end{block}\end{textblock}

Regularized empirical risk minimization problem:
$$\min_{\bm{G}} \frac{1}{Nn}\|\bm{Y} - \hat{\bm{Y}}\bm{G}'\bm{S}'\|_F + \lambda \|\text{vec}{\bm{G}}\|_1,
$$

* $N=T-T_1-h+1$,\quad $T_1$ is minimum training sample size
* $\|\cdot\|_F$ is the Frobenius norm
* $\bm{Y} = [\bm{y}_{T_1+h}, \dots, \bm{y}_{T}]'$
* $\hat{\bm{Y}} = [\hat{\bm{y}}_{T_1+h|T_1}, \dots, \hat{\bm{y}}_{T|T-h}]'$
* $\lambda$ is a regularization parameter.

When $\lambda=0$, $\hat{\bm{G}} = \bm{B}'\hat{\bm{Y}}(\hat{\bm{Y}}'\hat{\bm{Y}})^{-1}$
where $\bm{B} = [\bm{b}_{T_1+h}, \dots, \bm{b}_{T}]'$.

Reference: @Ben_TaiEtAl2019

# Unconstrained MinT

## Unconstrained MinT

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{Wickramasuriya (2022)}
\end{block}\end{textblock}


# Adding optimization constraints

## Adding optimization constraints

Any approach to reconciliation based on optimisation uses a form of constrained optimisation since reconciled forecasts must lie on the coherent subspace. However, at times additional constraints may be implemented. The first is the case where reconciled forecasts must be non-negative. In general, even if base forecasts are constrained to be positive (which can be achieved by modelling on the log scale and back-transforming), there is no guarantee that the usual reconciliation approaches such as OLS and MinT will maintain the non-negativity of forecasts. To address this issue, the usual optimisation problem can be augmented with non-negativity constraints on the reconciled forecasts. Such optimisation problems can be solved using quadratic programming, with @WicEtAl2020 providing an early example for forecast reconciliation, and @di2023spatio a more recent example.

@KouAth2021 also consider the case of non-negative reconciled forecasts. However, instead of using a constrained optimisation approach, they propose a heuristic to iteratively adjust the reconciled predictions to be non-negative. Although this does not guarantee optimal solutions, their proposed algorithm has the interesting feature that it distributes adjustments of forecasts across the hierarchy, which can be useful in a variety of situations, such as the application of judgemental adjustments on specific nodes of the hierarchy.

@di2023spatio also discuss an effective nonnegative heuristic called "set-negative-to-zero", whereby the negative reconciled forecasts at the bottom level are set to zero, and the remaining forecasts computed via aggregation.

Another constraint of interest is where some particular base forecasts remain unchanged.  For instance, @HolEtAl2021 consider the case of reconciliation where the top-level base forecast is retained. This differs from truly top-down approaches in that it can be done while also preserving the unbiasedness of base forecasts. To briefly illustrate the main idea, for a three variable hierarchy where $y_{Tot,t}=y_{A,t}+y_{B,t}$, either setting
$$
  \begin{pmatrix}\tilde{y}_{Tot,t}\\\tilde{y}_{A,t}\\\tilde{y}_{B,t}\end{pmatrix} =
  \begin{pmatrix}\hat{y}_{Tot,t}\\\hat{y}_{A,t}\\\hat{y}_{Tot,t}-\hat{y}_{A,t}\end{pmatrix}
  \qquad\textrm{or}\qquad
  \begin{pmatrix}\tilde{y}_{Tot,t}\\\tilde{y}_{A,t}\\\tilde{y}_{B,t}\end{pmatrix} =
  \begin{pmatrix}\hat{y}_{Tot,t}\\\hat{y}_{Tot,t}-\hat{y}_{B,t}\\\hat{y}_{B,t}\end{pmatrix}
$$
will lead to coherent forecasts that preserve unbiasedness. Any average between these two solutions will have the same properties. @HolEtAl2021 generalise this idea to more complex hierarchies, and the properties of their methods are investigated by @Di_FonGir2022b. @ZhaEtAl2022 further generalise this idea to a setting where reconciliation can be carried out while keeping a subset of base forecasts unchanged and not just the top level. Conditions on how a set of such "immutable" series can be selected are also provided by @ZhaEtAl2022.

## ML and regularization

## Bayesian versions

## In-built coherence

\nocite{htsgeometry, nonnegmint, Di_FonGir2022b,swissexports,Van_ErvCug2015}
