---
title: Forecast reconciliation
subtitle: 2. Perspectives on forecast reconciliation
author: Rob J Hyndman
pdf-engine: pdflatex
fig-width: 9
fig-height: 4.5
format:
  beamer:
    theme: monash
    aspectratio: 169
    fontsize: 14pt
    section-titles: false
    knitr:
      opts_chunk:
        dev: "CairoPDF"
include-in-header: header.tex
cite-method: biblatex
bibliography: hts.bib
biblio-title: References
keep-tex: true
execute:
  echo: false
  message: false
  warning: false
  cache: true
---

```{r}
#| cache: false
source("setup.R")
```

## Outline

\vspace*{0.7cm}\tableofcontents

# Time series reconciliation

## Time series reconciliation
\fontsize{13}{15}\sf

* @Stone1942: reconciling national economic accounts (disaggregated into production, income, outlay, capital transactions, etc.)
* @Byron1978: extended Stone's work using more computationally efficient methods.
* 1984: Stone wins Nobel Prize in Economics.
* Same approach used for reconciling seasonally adjusted data.
* @chow1971best: Temporal reconciliation of monthly or quarterly estimates to sum to annual estimates.
* @Di_Fonzo1990: Cross-temporal reconciliation of time series data.

# Reconciliation via constraints

## Notation reminder

\begin{textblock}{8.5}(0.2,1.5)
Every collection of time series with linear constraints can be written as
\centerline{\colorbox[RGB]{210,210,210}{$\bY_{t}=\color{blue}\bS\color{red}\bm{b}_{t}$}}
\vspace*{-0.9cm}\begin{itemize}\parskip=0cm\itemsep=0cm
\item $\by_t=$ vector of all series at time $t$
\item $ y_{\text{Total},t}= $ aggregate of all series at time
$t$.
\item $ y_{X,t}= $ value of series $X$ at time $t$.
\item $\color{red}{\bm{b}_t}=$ vector of most disaggregated series at time $t$
\item $\color{blue}{\bS}=$ ``summing matrix'' containing the linear constraints.
\end{itemize}
\end{textblock}

\begin{textblock}{5.7}(11.4,0.1)
\begin{minipage}{4cm}
\begin{block}{}\centering
\begin{tikzpicture}
\tikzstyle{every node}=[ellipse,draw,fill=red!15,inner sep=2pt]
\tikzstyle[level distance=.3cm]
\tikzstyle[sibling distance=12cm]
\tikzstyle{level 1}=[sibling distance=10mm,font=\small,set style={{every node}+=[fill=blue!15]}]
\node{Total}[edge from parent fork down]
 child {node {A}
 }
 child {node {B}
 }
 child {node {C}
 };
\end{tikzpicture}
\end{block}
\end{minipage}
\end{textblock}

\only<1>{\begin{textblock}{5.7}(9.4,2.8)\fontsize{14}{15}\sf
\begin{align*}
\bY_{t}&= \begin{pmatrix}
  y_{\text{Total},t}\\
  y_{A,t}\\
  y_{B,t}\\
  y_{C,t}
  \end{pmatrix}  \\
  &= {\color{blue}\underbrace{\begin{pmatrix}
                1 & 1 & 1 \\
                1 & 0 & 0 \\
                0 & 1 & 0\\
                0 & 0 & 1
                \end{pmatrix}}_{\bS}}
     {\color{red}\underbrace{\begin{pmatrix}
       y_{A,t}\\y_{B,t}\\y_{C,t}
       \end{pmatrix}}_{\bm{b}_{t}}}
\end{align*}
\end{textblock}}
\only<2>{\begin{textblock}{5.7}(9.4,3.2)\fontsize{14}{15}\sf
\begin{alertblock}{}
\begin{itemize}\itemsep=0.1cm
\item Base forecasts: $\hat{\bm{y}}_{T+h|T}$
\item Reconciled forecasts: $\tilde{\bm{y}}_{T+h|T}=\bS\bm{G}\hat{\bm{y}}_{T+h|T}$
\item MinT: $\bG = (\bS'\bm{W}_h^{-1}\bS)^{-1}\bS'\bm{W}_h^{-1}$ where $\bm{W}_h$ is covariance matrix of base forecast errors.
\end{itemize}
\end{alertblock}
\end{textblock}}

## Notation

\begin{textblock}{5.7}(11.4,0.1)
\begin{minipage}{4cm}
\begin{block}{}\centering
\begin{tikzpicture}
\tikzstyle{every node}=[ellipse,draw,fill=red!15,inner sep=2pt]
\tikzstyle[level distance=.3cm]
\tikzstyle[sibling distance=12cm]
\tikzstyle{level 1}=[sibling distance=10mm,font=\small,set style={{every node}+=[fill=blue!15]}]
\node{Total}[edge from parent fork down]
 child {node {A}
 }
 child {node {B}
 }
 child {node {C}
 };
\end{tikzpicture}
\end{block}
\end{minipage}
\end{textblock}

\begin{textblock}{6}(0.5,1.5)
\begin{block}{Aggregation matrix}\vspace*{-0.6cm}
\begin{align*}
\bY_{t} & =\color{blue}\bS\color{red}\bm{b}_{t} \\[0.3cm]
\begin{pmatrix}
   \textcolor{DarkYellow}{y_{\text{Total},t}}\\
   \textcolor{red}{y_{A,t}}\\
   \textcolor{red}{y_{B,t}}\\
   \textcolor{red}{y_{C,t}}
  \end{pmatrix}
  &= {\color{blue}\begin{pmatrix}
                \textcolor{DarkYellow}1 & \textcolor{DarkYellow}1 & \textcolor{DarkYellow}1 \\
                1 & 0 & 0 \\
                0 & 1 & 0\\
                0 & 0 & 1
                \end{pmatrix}}
     {\color{red}\begin{pmatrix}
       y_{A,t}\\y_{B,t}\\y_{C,t}
       \end{pmatrix}} \\[0.2cm]
  \begin{pmatrix}\textcolor{DarkYellow}{\bm{a}_t}\\\textcolor{red}{\bm{b}_t}\end{pmatrix}
   & = \begin{pmatrix}\textcolor{DarkYellow}{\bm{A}}\\\bm{I}_{n_b}\end{pmatrix}\textcolor{red}{\bm{b}_t}
\end{align*}
\end{block}
\end{textblock}

\begin{textblock}{7}(8.1,3.5)
\begin{block}{Constraint matrix}\vspace*{-0.6cm}
\begin{align*}
\bm{C} \bY_t & = \bm{0} \\
\text{where}\qquad
\bm{C} & =  \begin{bmatrix} 1 & -1 & -1 & -1 \end{bmatrix} \\
  & = \begin{bmatrix} \bm{I}_{n_a} & -\textcolor{DarkYellow}{\bm{A}} \end{bmatrix}
\end{align*}
\end{block}
\end{textblock}


## Zero-constraint representation
\vspace*{0.2cm}

\begin{block}{Aggregation matrix $\bm{A}$}
$$\bm{y}_t
    = \begin{bmatrix}\bm{a}_t\\\bm{b}_t\end{bmatrix}
    = \begin{bmatrix}\bm{A}\\\bm{I}_{n_b}\end{bmatrix}\bm{b}_t
    = \bm{S}\bm{b}_t
$$
\end{block}\pause

\begin{block}{Constraint matrix $\bm{C}$}
\centerline{$\bm{C}\bm{y}_t = \bm{0}$}
\end{block}\vspace*{-0.3cm}

* Constraint matrix approach more general & more parsimonious.
* $\bm{C} = [\bm{I}_{n_a} ~~~ {-\bm{A}}]$.
* $\bm{S}$, $\bm{A}$ and $\bm{C}$ may contain any real values (not just 0s and 1s).

## Zero-constraint representation

Assuming $\bm{C}$ is full rank
\begin{block}{}
\centerline{$\tilde{\bm{y}}_{T+h|T} = \bm{M}\hat{\bm{y}}_{T+h|T}$}
\centerline{where\qquad $\bm{M} = \bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}$}
\end{block}\vspace*{-0.3cm}

* Originally proved by @Byron1978 for reconciling data.
* Re-discovered by @mint for reconciling forecasts.
* $\bm{M} = \bm{S}\bm{G}$ (the MinT solution)
* Leads to more efficient reconciliation than using $\bm{G}$.

## Zero-constraint representation

\begin{textblock}{5.7}(11.4,0.1)
\begin{minipage}{4cm}
\begin{block}{}\centering
\begin{tikzpicture}
\tikzstyle{every node}=[ellipse,draw,fill=red!15,inner sep=2pt]
\tikzstyle[level distance=.3cm]
\tikzstyle[sibling distance=12cm]
\tikzstyle{level 1}=[sibling distance=10mm,font=\small,set style={{every node}+=[fill=blue!15]}]
\node{Total}[edge from parent fork down]
 child {node {A}
 }
 child {node {B}
 }
 child {node {C}
 };
\end{tikzpicture}
\end{block}
\end{minipage}
\end{textblock}

\begin{textblock}{5.8}(9.6,3.5)
\begin{block}{}\vspace*{-0.6cm}\fontsize{11}{11}\sf
\begin{align*}
\bm{A} &= \begin{pmatrix}
           1 & 1 & 1
           \end{pmatrix} \\
\bS & = \begin{pmatrix}\bm{A}\\\bm{I}_{n_b}\end{pmatrix} =
\begin{pmatrix}
                1 & 1 & 1 \\
                1 & 0 & 0 \\
                0 & 1 & 0\\
                0 & 0 & 1
                \end{pmatrix} \\
\bm{C} & = \begin{pmatrix} \bm{I}_{n_a} & - \bm{A} \end{pmatrix} = \begin{pmatrix} 1 & -1 & -1 & -1 \end{pmatrix}
\end{align*}
\end{block}
\end{textblock}

\begin{textblock}{9}(.2, 1.5)\fontsize{11}{11}\sf
Suppose $\bm{W}_h = \bm{I}$. Then\vspace*{-0.2cm}
\begin{align*}
\bm{M} &= \bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} \\
       &= \begin{pmatrix}
                1 & 0 & 0 & 0 \\
                0 & 1 & 0 & 0 \\
                0 & 0 & 1 & 0\\
                0 & 0 & 0 & 1
                \end{pmatrix} -
                \begin{pmatrix} \phantom{-}1 \\ -1 \\ -1 \\ -1 \end{pmatrix} \frac{1}{4} \begin{pmatrix} 1 & -1 & -1 & -1 \end{pmatrix} \\
&= \begin{pmatrix}
                1 & 0 & 0 & 0 \\
                0 & 1 & 0 & 0 \\
                0 & 0 & 1 & 0\\
                0 & 0 & 0 & 1
                \end{pmatrix} -
                \begin{pmatrix}
                \phantom{-}1 & -1 & -1 & -1 \\
                -1 & \phantom{-}1 & \phantom{-}1 & \phantom{-}1 \\
                -1 & \phantom{-}1 & \phantom{-}1 & \phantom{-}1 \\
                -1 & \phantom{-}1 & \phantom{-}1 & \phantom{-}1
                \end{pmatrix} \\
                & =
                \begin{pmatrix}
                \frac{3}{4} & \phantom{-}\frac14 & \phantom{-}\frac14 & \phantom{-}\frac14 \\[0.1cm]
                \frac{1}{4} & \phantom{-}\frac34 & -\frac14 & -\frac14 \\[0.1cm]
                \frac{1}{4} & -\frac14 & \phantom{-}\frac34 & -\frac14  \\[0.1cm]
                \frac{1}{4} & -\frac14 & -\frac14 & \phantom{-}\frac34
                \end{pmatrix}
\end{align*}
\end{textblock}

# Example: reconciling GDP forecasts

## Example: reconciling GDP forecasts

\only<1>{\full{IncomeApproach}}
\only<2>{\full{ExpenditureApproach}}
\only<3>{\full{GFCF}}
\only<4>{\full{HFCE}}

## Example: reconciling GDP forecasts

* No unique hierarchy.
* Several disaggregations with the same parent node
* Not possible to represent using structural $\bm{S}$ notation.
* Instead, we can use the constraint $\bm{C}$ notation.

## Example: reconciling GDP forecasts

\alert{Using structural notation:}
$$
\bm{y}^I_t = \begin{bmatrix}x_t \\\bm{a}^I_t \\ \bm{b}^I_t\end{bmatrix} = \bm{S}^I\bm{b}_t^I\qquad
\bm{y}^E_t = \begin{bmatrix}x_t \\\bm{a}^E_t \\ \bm{b}^E_t\end{bmatrix} = \bm{S}^E\bm{b}_t^E
$$
where
$$\bm{S}^I = \begin{bmatrix}\bm{1}_{10}' \\\bm{A}^I\\\bm{I}_{10}\end{bmatrix}\qquad
\bm{S}^E = \begin{bmatrix}\bm{1}_{53}' \\\bm{A}^E\\\bm{I}_{53}\end{bmatrix}$$

* Can reconcile both trees, but the totals won't be equal.

## Example: reconciling GDP forecasts

\alert{Using constraint notation:}
$$
  \bm{C}\bm{y}_t = \bm{0}
$$
where
$$ \bm{y}_t = \begin{bmatrix}x_t\\
 \bm{a}^I_t\\
 \bm{b}^I_t\\
 \bm{a}^E_t\\
 \bm{b}^E_t
 \end{bmatrix}
 \qquad\text{and}\qquad
  \bm{C} = \begin{bmatrix}
  1 & \bm{0}_5' & -\bm{1}_{10}' & \bm{0}_{26}' & \bm{0}_{53}' \\
  1 & \bm{0}_5' & \bm{0}_{10}' & \bm{0}_{26}' & -\bm{1}_{53}' \\
  \bm{0}_5 & \bm{I}_5 & - \bm{A}^I & \bm{0}_{5\times 26} & \bm{0}_{5\times 53} \\
  \bm{0}_{26} & \bm{0}_{26\times 5} & \bm{0}_{26\times10} & \bm{I}_{26} & -\bm{A}^E
  \end{bmatrix}
$$\vspace{-0.2cm}

Ref: @Bisaglia2020

# The geometry of forecast reconciliation

## The coherent subspace

\begin{textblock}{9}(.2,1)\fontsize{13}{13}\sf
\begin{block}{Coherent subspace}
$n_b$-dimensional linear subspace $\mathfrak{s}\subset \mathbb{\chi}^n$ for which linear constraints hold for all $\bm{y}\in\mathfrak{s}$.
\end{block}\vspace*{-0.3cm}
\begin{block}{Hierarchical time series}
An $n$-dimensional multivariate time series such that $\bm{y}_t\in\mathfrak{s}\quad\forall t$.
\end{block}\vspace*{-0.3cm}
\begin{block}{Coherent point forecasts}
$\tilde{\bm{y}}_{t+h|t}$ is \emph{coherent} if $\tilde{\bm{y}}_{t+h|t} \in \mathfrak{s}$.
\end{block}\vspace*{-0.2cm}
\end{textblock}
\only<2-3>{\begin{textblock}{7.5}(.2,6.75)\fontsize{13}{13}\sf
\begin{alertblock}{Base forecasts}
Let $\hat{\bm{y}}_{t+h|t}$ be vector of \emph{incoherent} initial $h$-step forecasts.$\phantom{y_{t|h}}$
\end{alertblock}
\end{textblock}}
\only<3>{\begin{textblock}{7.5}(8.3,6.75)\fontsize{13}{13}\sf
\begin{alertblock}{Reconciled forecasts}
Let $\psi$ be a mapping, $\psi:\mathbb{\chi}^n\rightarrow\mathfrak{s}$.  $\tilde{\bm{y}}_{t+h|t}=\psi(\hat{\bm{y}}_{t+h|t})$ ``reconciles'' $\hat{\bm{y}}_{t+h|t}$.
\end{alertblock}
\end{textblock}}

\placefig{9.4}{.0}{width=6.6cm}{3D_hierarchy}
\begin{textblock}{3}(11.4,5.6)\fontsize{13}{13}\sf
\begin{block}{}
\centerline{$ y_{Tot} = y_A + y_B$}
\end{block}
\end{textblock}

## The coherent subspace

\begin{textblock}{5.7}(11.4,0.1)
\begin{minipage}{4cm}
\begin{block}{}\centering
\begin{tikzpicture}
\tikzstyle{every node}=[ellipse,draw,fill=red!15,inner sep=2pt]
\tikzstyle[level distance=.3cm]
\tikzstyle[sibling distance=12cm]
\tikzstyle{level 1}=[sibling distance=10mm,font=\small,set style={{every node}+=[fill=blue!15]}]
\node{Total}[edge from parent fork down]
 child {node {A}
 }
 child {node {B}
 }
 child {node {C}
 };
\end{tikzpicture}
\end{block}
\end{minipage}
\end{textblock}

The columns of $\bm{S}$ form a basis set for $\mathfrak{s}$.

They are not unique.\newline Each corresponds to different vector of "bottom-level" series.

\only<2>{\begin{block}{}
\centerline{$\displaystyle
\bm{y} = \begin{pmatrix}\text{Total}\\A\\B\\C\end{pmatrix}\qquad
\bS  = \begin{pmatrix*}[r]
                \phantom{-}1 & \phantom{-}1 & \phantom{-}1 \\
                1 & 0 & 0 \\
                0 & 1 & 0\\
                0 & 0 & 1
                \end{pmatrix*} \qquad
  \bm{b} = \begin{pmatrix}\phantom{Total}\\[-0.7cm]
                A \\
                B \\
                C \\
                \end{pmatrix}
$}
\end{block}}
\only<3>{\begin{block}{}
\centerline{$\displaystyle
\bm{y} = \begin{pmatrix}\text{Total}\\A\\B\\C\end{pmatrix}\qquad
\bS  =
\begin{pmatrix*}[r]
                \phantom{-}1 & 0 & 0 \\
                0 & 0 & 1 \\
                0 & 1 & 0\\
                1 & -1 & -1
                \end{pmatrix*}\qquad
  \bm{b} = \begin{pmatrix}
      \text{Total} \\
      B \\
      A
  \end{pmatrix}
$}
\end{block}}
\only<4>{\begin{block}{}
\centerline{$\displaystyle
\bm{y} = \begin{pmatrix}\text{Total}\\A\\B\\C\end{pmatrix}\qquad
\bS  =
\begin{pmatrix*}[r]
                1 & 0 & 0 \\
                1 & 0 & -1 \\
                -1 & 1 & 1\\
                1 & -1 & 0
                \end{pmatrix*}\qquad
  \bm{b} = \begin{pmatrix}
      \text{Total} \\
      B+A \\
      C+B
  \end{pmatrix}
$}
\end{block}}

\vspace*{10cm}

## Projections in linear algebra

\begin{textblock}{9}(.2,1.25)
\begin{tikzpicture}
  % Define coordinates
  \coordinate (O) at (0,0);
  \coordinate (A) at (4,2);
  \coordinate (B) at (2,3);
  \coordinate (L) at (1,5);

  % Draw axes
  \draw[->,thick] (0,0) -- (5,0) node[anchor=north] {$x$};
  \draw[->,thick] (0,0) -- (0,4) node[anchor=east] {$y$};

  % Draw light
  \path [fill = yellow!30!white] (L) circle (.9cm);
  \path [fill = yellow!60!white] (L) circle (.6cm);
  \path [fill = yellow!90!white] (L) circle (.3cm);

  % Draw vectors
  \draw[->,thick,red] (O) -- (A);
  \draw[->,thick,blue] (O) -- (B);

  % Draw projection
  \coordinate (P) at ($(A)!(B)!(O)$);
  \only<2->{\draw[->,thick,blue,dashed] (B) -- (P);}
  \only<2->{\draw[->,thick,blue, dashed] (O) -- (P);}
\end{tikzpicture}
\end{textblock}

```{r}
#| include: false
library(scatterplot3d)
Cairo::CairoPDF(file = "figs/scatterplot1.pdf", width = 7, height = 7)
s3d <- scatterplot3d(trees,
  type = "p", color = "blue",
  angle = 55, pch = 16
)
crop::dev.off.crop()
Cairo::CairoPDF(file = "figs/scatterplot2.pdf", width = 7, height = 7)
s3d <- scatterplot3d(trees,
  type = "h", color = "blue",
  angle = 55, pch = 16
)
for (i in seq(NROW(trees))) {
  s3d$points3d(trees$Girth[i], trees$Height[i], 10,
    col = "red", type = "p", pch = 16, cex = .75
  )
}
crop::dev.off.crop()
Cairo::CairoPDF(file = "figs/scatterplot3.pdf", width = 7, height = 7)
s3d <- scatterplot3d(trees,
  type = "p", color = "blue",
  angle = 55, pch = 16
)
# Add regression plane
my.lm <- lm(trees$Volume ~ trees$Girth + trees$Height)
s3d$plane3d(my.lm, col = "red")
crop::dev.off.crop()
Cairo::CairoPDF(file = "figs/scatterplot4.pdf", width = 7, height = 7)
s3d <- scatterplot3d(trees,
  type = "p", color = "blue",
  angle = 55, pch = 16
)
# Add regression plane
my.lm <- lm(trees$Volume ~ trees$Girth + trees$Height)
s3d$plane3d(my.lm, col = "red")
fits <- fitted(my.lm)
for (i in seq(NROW(trees))) {
  s3d$points3d(
    rep(trees$Girth[i], 2),
    rep(trees$Height[i], 2),
    c(fits[i], trees$Volume[i]),
    col = "blue", type = "l"
  )
  s3d$points3d(
    trees$Girth[i], trees$Height[i], fits[i],
    col = "red", type = "p", cex = .75, pch = 16
  )
}
crop::dev.off.crop()
system("pdfcrop figs/scatterplot1.pdf figs/scatterplot1.pdf")
system("pdfcrop figs/scatterplot2.pdf figs/scatterplot2.pdf")
system("pdfcrop figs/scatterplot3.pdf figs/scatterplot3.pdf")
system("pdfcrop figs/scatterplot4.pdf figs/scatterplot4.pdf")
```

\begin{textblock}{10}(7,1.6)
\only<3>{\includegraphics[width=8.8cm]{scatterplot1}}
\only<4>{\includegraphics[width=8.8cm]{scatterplot2}}
\only<5>{\includegraphics[width=8.8cm]{scatterplot3}}
\only<6>{\includegraphics[width=8.8cm]{scatterplot4}}
\end{textblock}

## Projections in linear algebra

* A projection is a linear transformation $\bm{M}$ such that $\bm{M}^2=\bm{M}$.
* i.e., $M$ is idempotent: it leaves its image unchanged.
* $\bm{M}$ projects onto $\mathfrak{s}$ if $\bm{M}\bm{y}=\bm{y}$ for all $\bm{y}\in\mathfrak{s}$.
* All eigenvalues of $\bm{M}$ are either 0 or 1.
* All singular values of $\bm{M}$ are greater than or equal to 1 (with equality iff $\bm{M}$ is orthogonal).
* A projection is *orthogonal* if $\bm{M}'=\bm{M}$.
* If a projection is not orthogonal, it is called *oblique*.
* In regression, OLS is an orthogonal projection onto space spanned by predictors.

## Linear projection reconciliation

\only<1>{\placefig{9.5}{1.5}{width=6.2cm}{InsampDir_2_George}}
\only<2>{\placefig{9.5}{1.5}{width=6.2cm}{OrthProj_George}}
\only<3->{\placefig{9.5}{1.5}{width=6.2cm}{ObliqProj_George}}

\begin{textblock}{9}(.5,1.5)
  \begin{itemize}\tightlist
  \item $R$ is the most likely direction of deviations from $\mathfrak{s}$.
  \only<1->{\item Grey: potential base forecasts}
  \only<2->{\item Red: reconciled forecasts}
  \only<2->{\item Orthogonal projections (i.e., OLS) lead to smallest possible adjustments of base forecasts.}
  \only<3->{\item Oblique projections (i.e., MinT) give reconciled forecasts with smallest variance.}
  \end{itemize}
\end{textblock}

\only<2->{
  \begin{textblock}{4.5}(11.2,7.6)\fontsize{13}{14}\sf
  \begin{block}{}
  \only<2>{Orthogonal projection}
  \only<3>{Oblique projection}
  \end{block}
  \end{textblock}
}

## Linear projection reconciliation
\fontsize{14}{16}\sf
\vspace*{0.2cm}\begin{alertblock}{}
\centerline{$\tilde{\bm{y}}_{t+h|t}= \psi(\hat{\bm{y}}_{t+h|t}) = \bm{M}\hat{\bm{y}}_{t+h|t}$}
\end{alertblock}\vspace*{-0.5cm}

* $\bm{M}$ is a projection onto $\mathfrak{s}$ if and only if $\bm{M}\bm{y}=\bm{y}$ for all $\bm{y}\in\mathfrak{s}$.
* Coherent base forecasts are unchanged since $\bm{M}\hat{\bm{y}}=\hat{\bm{y}}$
* If $\hat{\bm{y}}$ is unbiased, then $\tilde{\bm{y}}$ is also unbiased since
$$
  \E(\tilde{\bm{y}}_{t+h|t}) = \E(\bm{M}\hat{\bm{y}}_{t+h|t}) = \bm{M} \E(\hat{\bm{y}}_{t+h|t}) = \E(\hat{\bm{y}}_{t+h|t}),
$$
and unbiased estimates must lie on $\mathfrak{s}$.
* The projection is orthogonal if and only if $\bm{M}'=\bm{M}$.
* If $\bm{S}$ forms a basis set for $\mathfrak{s}$, then projections are of the form $\bm{M} = \bS(\bS'\bm{\Psi}\bS)^{-1}\bS'\bm{\Psi}$ where $\bm{\Psi}$ is a positive definite matrix.

\vspace*{10cm}

## Linear projection reconciliation

\vspace*{0.2cm}\begin{alertblock}{}
\centerline{$\tilde{\bm{y}}_{t+h|t}= \psi(\hat{\bm{y}}_{t+h|t}) = \bm{M}\hat{\bm{y}}_{t+h|t},\qquad\text{where}\quad \bm{M} = \bS(\bS'\bm{\Psi}\bS)^{-1}\bS'\bm{\Psi}$}
\end{alertblock}\vspace*{.01cm}
\begin{block}{}\vspace*{-0.6cm}
\begin{align*}
&\text{OLS:}  && \bm{\Psi}=\bm{I} &&& \bm{M} & = \bm{S}(\bm{S}'\bm{S})^{-1}\bm{S}' && = \bm{I} - \bm{C}'(\bm{C}\bm{C}')^{-1}\bm{C} \\
&\text{MinT:} && \bm{\Psi}=\bm{W}_h &&&\bm{M} & = \bS(\bS'\bm{W}_h^{-1}\bS)^{-1}\bS'\bm{W}_h^{-1} && = \bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}
\end{align*}
\end{block}\vspace*{-0.3cm}

* $\bm{M}$ is orthogonal iff $\bm{\Psi}=\bm{I}$.
* $\bm{W}_h = \var[\by_{T+h} - \hat{\by}_{T+h|T} \mid \by_1,\dots,\by_T]$ is the covariance matrix of the base forecast errors.
* $\bm{V}_h = \var[\by_{T+h} - \tilde{\by}_{T+h|T}  \mid \by_1,\dots,\by_T]  = \bm{M}\bm{W}_h\bm{M}'$ is minimized when  $\bm{\Psi} = \bm{W}_h$.

\vspace*{10cm}

## Mean square error bounds

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{htsgeometry}
\end{block}\end{textblock}

\vspace*{0.2cm}\begin{alertblock}{Distance reducing property}
Let $\|\bm{u}\|_{\bm{\Psi}} = \bm{u}'\bm{\Psi}\bm{u}$. Then
  \centerline{$\|\bm{y}_{t+h}-\tilde{\bm{y}}_{t+h|t}\|_{\bm{\Psi}}\le\|\bm{y}_{t+h}-\hat{\bm{y}}_{t+h|t}\|_{\bm{\Psi}}$}
\end{alertblock}

 * $\bm{\Psi}$-projection is guaranteed to improve forecast accuracy over base forecasts *using this distance measure*.
 * Distance reduction holds for any realisation and any forecast.
 * OLS reconciliation minimizes Euclidean distance.
 * Other measures of forecast accuracy may be worse.

## Mean square error bounds

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{wickramasuriya2021properties}
\end{block}\end{textblock}

\begin{block}{}\vspace*{-0.6cm}
\begin{align*}
\|\bm{y}_{t+h} - \tilde{\bm{y}}_{t+h}\|_2^2
 &= \|\bm{M}(\bm{y}_{t+h} - \hat{\bm{y}}_{t+h})\|_2^2 \\
 &\le \|\bm{M}\|_2^2 \|\bm{y}_{t+h} - \hat{\bm{y}}_{t+h}\|_2^2 \\
 & = \sigma_{\text{max}}^2\|\bm{y}_{t+h} - \hat{\bm{y}}_{t+h}\|_2^2
\end{align*}
\end{block}

 * $\sigma_{\text{max}}$ is the largest eigenvalue of $\bm{M}$
 * $\sigma_{\text{max}}\ge1$ as $\bm{M}$ is a projection matrix.
 * Every projection reconciliation is better than base forecasts using Euclidean distance.

## Mean square error bounds

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{wickramasuriya2021properties}
\end{block}\end{textblock}
\vspace*{0.2cm}\begin{block}{}\vspace*{-0.6cm}
\begin{align*}
    & \text{tr}\Big(\E[\bm{y}_{t+h} - \tilde{\bm{y}}^{\text{MinT}}_{t+h|t}]'[\bm{y}_{t+h} - \tilde{\bm{y}}^{\text{MinT}}_{t+h|t}]\Big) \\
\le~ & \text{tr}\Big(\E[\bm{y}_{t+h} - \tilde{\bm{y}}^{\text{OLS}}_{t+h|t}]'[\bm{y}_{t+h} - \tilde{\bm{y}}^{\text{OLS}}_{t+h|t}]\Big) \\
\le~ & \text{tr}\Big(\E[\bm{y}_{t+h} - \hat{\bm{y}}_{t+h|t}]'[\bm{y}_{t+h} - \hat{\bm{y}}_{t+h|t}]\Big)
\end{align*}
\end{block}

Using sums of variances:

* MinT reconciliation is better than OLS reconciliation
* OLS reconciliation is better than base forecasts

# Optimization and reconcilation

## Minimum trace reconciliation

\vspace*{0.2cm}\begin{alertblock}{Minimum trace (MinT) reconciliation}
If $\bS\bG$ is a projection, then the trace of $\bm{V}_h = \text{Var}(\tilde{\bm{y}}_{t+h|t} - \bm{y}_{t+h})$ is \textbf{minimized} when
\centerline{$\bG = (\bS'\bm{W}_h^{-1}\bS)^{-1}\bS'\bm{W}_h^{-1}$}
\end{alertblock}
\begin{block}{}
\centerline{$\displaystyle\textcolor{red}{\tilde{\by}_{T+h|T}}
=\bS(\bS'\bm{W}_h^{-1}\bS)^{-1}\bS'\bm{W}_h^{-1}\textcolor{blue}{\hat{\by}_{T+h|T}}$}
\end{block}
\centerline{\hspace*{1cm}\textcolor{red}{Reconciled forecasts}\hfill\textcolor{blue}{Base forecasts}\hspace*{2cm}}

* Trace of $\bm{V}_h$ is sum of forecast variances.
* MinT solution is $L_2$ **optimal** amongst linear unbiased forecasts.

## A game theoretic perspective
\fontsize{14}{16}\sf

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{Van_ErvCug2015}
\end{block}\end{textblock}

Find the solution to the minimax problem
$$
V = \mathop{min}_{\tilde{\bm{y}} \in \mathfrak{s}}
\mathop{max}_{\bm{y}\in \mathfrak{s}}
\left\{\ell(\bm{y},\tilde{\bm{y}}) -
\ell(\bm{y},\hat{\bm{y}})
\right\},
$$
where $\ell$ is a loss function, and $\mathfrak{s}$ is the coherent subspace.

* $V\le0$: reconciliation guaranteed to reduce loss.
* If $\ell(\bm{y},\tilde{\bm{y}}) = \|\bm{y}- \tilde{\bm{y}}\|_{\bm{\Psi}} = (\bm{y}-\tilde{\bm{y}})'\bm{\Psi}(\bm{y}-\tilde{\bm{y}})$, where $\bm{\Psi}$ is any symmetric pd matrix, then:

  1. $\tilde{\bm{y}}=\bm{S}(\bm{S}'\bm{\Psi}\bm{S})^{-1}\bm{S}'\bm{\Psi}\hat{\bm{y}}$ will always improve upon the base forecasts;
  2. The MinT solution $\tilde{\bm{y}}=\bm{S}(\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}\bm{S}'\bm{W}_h^{-1}\hat{\bm{y}}$ will optimise loss in expectation over any choice of $\bm{\Psi}$.

## Biased reconciliation

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{Ben_TaiEtAl2019}
\end{block}\end{textblock}

Regularized empirical risk minimization problem:
$$\min_{\bm{G}} \frac{1}{Nn}\|\bm{Y} - \hat{\bm{Y}}\bm{G}'\bm{S}'\|_F + \lambda \|\text{vec}{\bm{G}}\|_1,
$$

* $N=T-T_1-h+1$,\quad $T_1$ is minimum training sample size
* $\|\cdot\|_F$ is the Frobenius norm
* $\bm{Y} = [\bm{y}_{T_1+h}, \dots, \bm{y}_{T}]'$
* $\hat{\bm{Y}} = [\hat{\bm{y}}_{T_1+h|T_1}, \dots, \hat{\bm{y}}_{T|T-h}]'$
* $\lambda$ is a regularization parameter

When $\lambda=0$:\qquad  $\hat{\bm{G}} = \bm{B}'\hat{\bm{Y}}(\hat{\bm{Y}}'\hat{\bm{Y}})^{-1}$
where $\bm{B} = [\bm{b}_{T_1+h}, \dots, \bm{b}_{T}]'$.

Reference: @Ben_TaiEtAl2019

## MinT expressed as a regression

Since $\tilde{\bm{b}}_{t+h|t} = (\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}\bm{S}'\bm{W}_h^{-1}\hat{\bm{y}}_{t+h|t}$, we can write the MinT solution as a regression problem:
\begin{block}{}\vspace*{-0.7cm}
\begin{align*}
\tilde{\bm{b}}_{t+h|t} &= \text{arg min}_{\bm{b}}\big[\hat{\bm{y}}_{t+h|t} - \bm{S}\bm{b}\big]' \bm{W}_h^{-1}\big[\hat{\bm{y}}_{t+h|t} - \bm{S}\bm{b}\big] \\
& =\text{arg min}_{\bm{b}}\big[\bm{b}'\bm{S}'\bm{W}_h^{-1}\bm{S}\bm{b} - 2\bm{b}'\bm{S}'\bm{W}_h^{-1}\hat{\bm{y}}_{t+h|t} +
\hat{\bm{y}}_{t+h|t}'\bm{W}_h^{-1}\hat{\bm{y}}_{t+h|t} \big]\\
& =\text{arg min}_{\bm{b}}\big[\bm{b}'\bm{S}'\bm{W}_h^{-1}\bm{S}\bm{b} - 2\bm{b}'\bm{S}'\bm{W}_h^{-1}\hat{\bm{y}}_{t+h|t}\big]
\end{align*}
\end{block}\vspace*{-0.2cm}

* MinT solution is equivalent to a GLS regression of $\hat{\bm{y}}_{t+h|t}$ on $\bm{S}$ with covariance weights $\bm{W}_h^{-1}$.
* The estimated coefficients are the forecasts of the bottom level series.

\vspace*{10cm}

## Non-negative forecasts
\fontsize{14}{15}\sf
\vspace*{0.1cm}\begin{alertblock}{}
\centerline{$\min_{\bm{G}_h}\text{tr}\Big(\E[\bm{y}_{t+h} - \bm{S}\bm{G}_h\hat{\bm{y}}_{t+h|t}]'[\bm{y}_{t+h} - \bm{S}\bm{G}_h\hat{\bm{y}}_{t+h|t}]\Big)$}
\centerline{such that $\bm{b}_{t+h|t} = \bm{G}_h\hat{\bm{y}}_{t+h|t} \ge 0$}
\end{alertblock}\pause\vspace*{-0.2cm}

\begin{block}{Solve via quadratic programming:}
$$
  \text{min}_{\bm{b}} \big[\bm{b}'\bS'\bm{W}_h^{-1}\bS\bm{b} - 2 \bm{b}'\bS'\bm{W}_h^{-1}\hat{\bm{y}}_{T+h|T}\big] \quad \text{s.t.~~} \bm{b} \ge 0
$$
\rightline{\citep{nonnegmint}}
\end{block}\pause\vspace*{-0.1cm}

### Set-negative-to-zero heuristic solution
  * Negative reconciled forecasts at bottom level set to zero
  * Remaining forecasts computed via aggregation

\rightline{\citep{di2023spatio}}

## Immutable forecasts

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{ZhaEtAl2022}
\end{block}\end{textblock}
$\displaystyle \hat{\bm{y}}_{t+h|t} = \begin{bmatrix}\hat{\bm{a}}_{t+h|t} \\ \hat{\bm{v}}_{t+h|t} \\ \hat{\bm{u}}_{t+h|t}\end{bmatrix} = \begin{bmatrix} \bm{A}_1 & \bm{A}_2 \\ \bm{I}_{n_b-k} & \bm{O} \\ \bm{O} & \bm{I}_k \end{bmatrix} \begin{bmatrix}\hat{\bm{v}}_{t+h|t} \\ \hat{\bm{u}}_{t+h|t}\end{bmatrix}$\vspace*{-0.5cm}

Suppose $\hat{\bm{u}}_{t+h|t}$ are fixed and let $\hat{\bm{w}}_{t+h|t} = \begin{bmatrix}\hat{\bm{a}}_{t+h|t} - \bm{A}_2 \hat{\bm{u}}_{t+h|t} \\ \hat{\bm{v}}_{t+h|t}\end{bmatrix}$.
\begin{block}{Optimization problem}
\centerline{$\displaystyle\text{min}_{\bm{v}}\big[\hat{\bm{w}}_{t+h|t} - \bm{A}_3\bm{v}\big]' \bm{W}_{\bm{v}}^{-1}\big[\hat{\bm{w}}_{t+h|t} - \bm{A}_3\bm{v}\big]\qquad\text{where}\qquad
\bm{A}_3 = \begin{bmatrix}\bm{A}_1 \\\bm{I}_{n_b-k}\end{bmatrix}$}
and $\bm{W}_{\bm{v}}$ contains elements of $\bm{W}_h$ corresponding to $\hat{\bm{v}}_{t+h|t}$.
\end{block}
\vspace*{10cm}

## Immutable forecasts

\begin{textblock}{6.4}(9,0.)\begin{block}{}
\citet{ZhaEtAl2022}
\end{block}\end{textblock}
$\displaystyle \hat{\bm{y}}_{t+h|t} = \begin{bmatrix}\hat{\bm{a}}_{t+h|t} \\ \hat{\bm{v}}_{t+h|t} \\ \hat{\bm{u}}_{t+h|t}\end{bmatrix} = \begin{bmatrix} \bm{A}_1 & \bm{A}_2 \\ \bm{I}_{n_b-k} & \bm{O} \\ \bm{O} & \bm{I}_k \end{bmatrix} \begin{bmatrix}\hat{\bm{v}}_{t+h|t} \\ \hat{\bm{u}}_{t+h|t}\end{bmatrix}$\vspace*{-0.5cm}

Suppose $\hat{\bm{u}}_{t+h|t}$ are fixed and let $\hat{\bm{w}}_{t+h|t} = \begin{bmatrix}\hat{\bm{a}}_{t+h|t} - \bm{A}_2 \hat{\bm{u}}_{t+h|t} \\ \hat{\bm{v}}_{t+h|t}\end{bmatrix}$.
\begin{block}{Solve with non-negativity constraint}
\centerline{$\displaystyle\text{min}_{\bm{v}}\big[\hat{\bm{w}}_{t+h|t} - \bm{A}_3\bm{v}\big]' \bm{W}_{\bm{v}}^{-1}\big[\hat{\bm{w}}_{t+h|t} - \bm{A}_3\bm{v}\big]\qquad\text{where}\qquad
\bm{A}_3 = \begin{bmatrix}\bm{A}_1 \\\bm{I}_{n_b-k}\end{bmatrix}$}
$$\text{such that}\qquad \bm{A}_3 \bm{v} \ge \begin{bmatrix}-\bm{A}_2\hat{\bm{u}}_{t+h|t}\\ \bm{0}\end{bmatrix}$$
\end{block}

# Time series cross-validation

## Time series cross-validation

```{r tscvplots, echo=FALSE}
tscv_plot <- function(.init, .step, h = 1) {
  expand.grid(
    time = seq(26),
    .id = seq(trunc(15 / .step))
  ) |>
    group_by(.id) |>
    mutate(
      observation = case_when(
        time <= ((.id - 1) * .step + .init) ~ "train",
        time %in% c((.id - 1) * .step + .init + h) ~ "test",
        TRUE ~ "unused"
      )
    ) |>
    ungroup() |>
    filter(.id <= 26 - .init) |>
    ggplot(aes(x = time, y = .id)) +
    geom_segment(
      aes(x = 0, xend = 27, y = .id, yend = .id),
      arrow = arrow(length = unit(0.015, "npc")),
      col = "black", size = .25
    ) +
    geom_point(aes(col = observation), size = 2) +
    scale_y_reverse() +
    scale_color_manual(values = c(train = "#0072B2", test = "#D55E00", unused = "gray")) +
    # theme_void() +
    # geom_label(aes(x = 28.5, y = 1, label = "time")) +
    guides(col = FALSE) +
    labs(x = "time", y = "") +
    theme_void() +
    theme(axis.title = element_text())
}
```

**Traditional evaluation**

```{r traintest1, fig.height=.7, echo=FALSE, dependson="tscvplots"}
tscv_plot(.init = 18, .step = 10, h = 1:8) +
  geom_text(aes(x = 10, y = 0.6, label = "Training data"), color = "#0072B2") +
  geom_text(aes(x = 21, y = 0.6, label = "Test data"), color = "#D55E00") +
  ylim(1, 0)
```

\pause

**Time series cross-validation**

```{r tscvggplot1, echo=FALSE, fig.height=2.3}
tscv_plot(.init = 8, .step = 1, h = 1) +
  geom_text(aes(x = 21, y = 0, label = "h = 1"), color = "#D55E00")
```

## Time series cross-validation {-}

**Traditional evaluation**

```{r traintest2, ref.label="traintest1", fig.height=.7, echo=FALSE}
```

**Time series cross-validation**

```{r tscvggplot2, echo=FALSE,  dependson="tscvplots", fig.height=2.3}
tscv_plot(.init = 8, .step = 1, h = 2) +
  geom_text(aes(x = 21, y = 0, label = "h = 2"), color = "#D55E00")
```

## Time series cross-validation {-}

**Traditional evaluation**

```{r traintest3, ref.label="traintest1", fig.height=.7, echo=FALSE}
```

**Time series cross-validation**

```{r tscvggplot3, echo=FALSE,  dependson="tscvplots", fig.height=2.3}
tscv_plot(.init = 8, .step = 1, h = 3) +
  geom_text(aes(x = 21, y = 0, label = "h = 3"), color = "#D55E00")
```

## Time series cross-validation {-}

**Traditional evaluation**

```{r traintest4, ref.label="traintest1", fig.height=.7, echo=FALSE}
```

**Time series cross-validation**

```{r tscvggplot4, echo=FALSE,  dependson="tscvplots", fig.height=2.3}
tscv_plot(.init = 8, .step = 1, h = 4) +
  geom_text(aes(x = 21, y = 0, label = "h = 4"), color = "#D55E00")
```

\only<2>{\begin{textblock}{7.7}(.5,6.2)\begin{block}{}\fontsize{12}{13}\sf
\begin{itemize}\tightlist
\item Forecast accuracy averaged over test sets.
\item Also known as "evaluation on a rolling forecasting origin"
\end{itemize}\end{block}\end{textblock}}


# ML reconciliation

## ML reconciliation

\placefig{7}{0}{width=9.cm, height=20cm}{mlflow}

\begin{textblock}{7}(0,1.2)\fontsize{11}{12}\sf
\begin{enumerate}\tightlist
\item Split all series using time series cross-validation
\item For each training set, compute one-step-ahead forecasts for all series
\item For each bottom-level series, use RF or XGB to predict values using forecasts of all series as inputs
\item Forecast all series
\item For each bottom-level series, apply ML model to improve forecasts
\item Aggregate bottom-level forecasts to obtain forecasts for other series.
\end{enumerate}
\end{textblock}
\begin{textblock}{6,5}(0.5,7.8)\fontsize{11}{12}\sf
\begin{block}{}
Spiliotis et al. (ASC, 2021)\nocite{hfrml}
\end{block}
\end{textblock}

## ML reconciliation: tourism data

\fontsize{11}{11}\sf\hspace*{-0.8cm}
\begin{tabular}{l c c c c c}
    \textbf{Method} & \textbf{Total} & \textbf{States} & \textbf{Zones} & \textbf{Regions} & \textbf{Average} \\
    \hline
    \multicolumn{6}{c}{MASE} \\
    \hline
    MinT-Struct          & 1.094            & 0.968            & 0.887            & 0.843            & 0.948 \\
    MinT-Shrink & 1.047 & \textbf{0.956} & 0.872 & 0.824 & 0.925 \\
    ML-RF           & 1.045            & 0.964            & 0.859            & 0.812            & 0.920 \\
    ML-XGB          & \textbf{1.043}   & 0.965            & \textbf{0.859}   & \textbf{0.812}   & \textbf{0.920} \\
    \hline
    \multicolumn{6}{c}{RMSSE} \\
    \hline
    MinT-Struct          & 1.308            & 1.225            & 1.137            & 1.109            & 1.195 \\
    MinT-Shrink         & 1.265            & 1.214            & 1.120            & 1.086            & 1.171 \\
    ML-RF           & 1.261            & \textbf{1.208}   & 1.104            & 1.066            & 1.159 \\
    ML-XGB          & 1.255            & 1.208            & \textbf{1.101}   & \textbf{1.064}   & \textbf{1.157} \\
    \hline
    \multicolumn{6}{c}{AMSE} \\
    \hline
    MinT-Struct          & 0.988            & 0.611            & 0.426            & 0.349            & 0.593 \\
    MinT-Shrink         & 0.935            & 0.599            & 0.417            & 0.337            & 0.572 \\
    ML-RF           & 0.780            & \textbf{0.526}   & 0.366            & 0.319            & 0.498 \\
    ML-XGB          & \textbf{0.779}   & 0.526            & \textbf{0.365}   & \textbf{0.317}   & \textbf{0.497}
  \end{tabular}

\begin{textblock}{4.2}(11.2,2.4)
\begin{block}{}\fontsize{11}{11}\sf
\begin{itemize}\tightlist
\item ML methods not significantly different.
\item MinT methods significantly different from each other and from ML methods.
\end{itemize}\end{block}
\end{textblock}

## ML and regularization

@Mishchenko2019:\newline Optimize all forecasts with an incoherence penalty
$$
\text{min}_{\hat{\bm{y}}_{t}} \sum_{t=1}^T \|\bm{y}_t - \hat{\bm{y}}_t\|_2 + \lambda \sum_{t=1}^T \|\hat{\bm{y}}_t - \bm{S}_t \hat{\bm{b}}_t\|_2
$$\pause


@Shiratori2020:\newline Optimize bottom level forecasts with an incoherence penalty
$$
\text{min}_{\hat{\bm{b}}_{t}} \sum_{t=1}^T \|\hat{\bm{b}}_t - \bm{b}_t\|_2 + \sum_{t=1}^T \bm{\Lambda} \|\bm{a}_t - \bm{A}_t \hat{\bm{b}}_t\|_2
$$




# In-built coherence

## In-built coherence

**Two-step approach**: compute base forecasts $\hat{\bm{y}}_h$, and then reconcile them to produce $\tilde{\bm{y}}_h$.

**One-step approaches:** compute coherent $\tilde{\bm{y}}_h$ directly.

  * @lhf: linear regression models
  * @PenvanDal2017: state space models
  * @villegas2018supply: state space models

## In-built coherence using linear models
\fontsize{13}{14}\sf
Suppose $\hat{y}_{t,i} = \hat{\bm{\beta}}_{i}' \bm{x}_{t,i}$
with $\bm{x}_{t,i}= (1, x_{t,1,i},\dots,x_{t,p,i})$&nbsp; & &nbsp;\rlap{$\hat{\bm{y}}_i = (\hat{y}_{1,i}, \dots, \hat{y}_{T,i})$.}
\pause
\begin{align*}
\textcolor{blue}{\begin{pmatrix}
  \hat{\bm{y}}_1\\
  \hat{\bm{y}}_2\\
  \vdots\\
  \hat{\bm{y}}_n
  \end{pmatrix}} &=
  \textcolor{red}{\begin{pmatrix}
  \bm{X}_1 & 0        & \dots  & 0\\
  0        & \bm{X}_2 & \ddots & \vdots \\
  \vdots   & \ddots   & \ddots & 0\\
  0        & \dots    & 0      & \bm{X}_n
  \end{pmatrix}}
  \begin{pmatrix}
  \hat{\bm{\beta}}_1\\
  \hat{\bm{\beta}}_2\\
  \vdots\\
  \hat{\bm{\beta}}_n
\end{pmatrix},
&&\quad
  \bm{X}_i = \begin{pmatrix}
  1 & x_{1,i,1} & x_{1,i,2} & \dots & x_{1,i,p}\\
  1 & x_{2,i,1} & x_{2,i,2} & \dots & x_{2,i,p}\\
  \vdots & \vdots & \vdots & & \vdots \\
  1 & x_{T,i,1} & x_{T,i,2} & \dots & x_{T,i,p}
\end{pmatrix}
\end{align*}\pause
$\hat{\bm{B}} = (\textcolor{red}{\bm{X}}'\textcolor{red}{\bm{X}})^{-1} \textcolor{red}{\bm{X}}'\textcolor{blue}{\bm{Y}}$
\pause\qquad
$\hat{\bm{y}}_{t+h} = \bm{X}_{t+h}^* \hat{\bm{B}}$
\pause\qquad
$\bm{X}^*_{t+h} =\text{diag}(\bm{x}_{t+h,i}', \dots, \bm{x}_{t+h,n}')$
\pause
\begin{alertblock}{}\vspace*{-0.8cm}
\begin{align*}
\tilde{\bm{y}}_{t+h} & = \bm{S}(\bm{S}'\bm{W}_h\bm{S})^{-1}\bm{S}'\bm{W}_h
                            \hat{\bm{y}}_{t+h}
                         = \bm{S}(\bm{S}'\bm{W}_h\bm{S})^{-1}\bm{S}'\bm{W}_h
                            \bm{X}_{t+h}^* (\textcolor{red}{\bm{X}}'\textcolor{red}{\bm{X}})^{-1} \textcolor{red}{\bm{X}}'\textcolor{blue}{\bm{Y}}
\\
\bm{V}_h &= \sigma^2\bm{S}(\bm{S}'\bm{W}_h\bm{S})^{-1}\bm{S}'\bm{W}_h\big[1 + \bm{X}_{T+h}^*(\textcolor{red}{\bm{X}}'\textcolor{red}{\bm{X}})^{-1}(\bm{X}_{T+h}^*)'\big] \bm{W}_h\bm{S}'(\bm{S}'\bm{W}_h\bm{S})^{-1}\bm{S}'
\end{align*}
\end{alertblock}
\only<4->{\begin{alertblock}{}
Reference: \citet{lhf}
\end{alertblock}}

\vspace*{10cm}

## In-built coherence using state space models

@PenvanDal2017 propose the state space model
\begin{align*}
\bm{y}_t &= \bm{S}\bm{\mu}_t + \bm{Z}_t\bm{\beta} + \bm{\varepsilon}_t, && \bm{\varepsilon}_t \sim N(\bm{0}, \bm{\Sigma}_{\bm{\varepsilon}}),\\
\bm{\mu}_t &= \bm{\mu}_{t-1} + \bm{\eta}_t, && \bm{\eta}_t \sim N(\bm{0}, \bm{\Sigma}_{\bm{\eta}}).
\end{align*}

* Coherent forecasts arise naturally using the Kalman filter
* Covariance matrices difficult to estimate except for small hierarchies.
* Requires the same model for all series
* A related approach proposed by @villegas2018supply


\nocite{htsgeometry, nonnegmint, Di_FonGir2022b, Van_ErvCug2015, lhf}
